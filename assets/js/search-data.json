{
  
    
        "post0": {
            "title": "Deep Learning and Art Neural Style Transfer",
            "content": ". Introduction . Use two images a content image and style image to create a new image of the content image in the style of the style image. . Source used, Deep Learning Specialization Week 4 https://www.coursera.org/lecture/convolutional-neural-networks/what-is-neural-style-transfer-SA5H8 From original NST paper published by the Visual Geometry Group at University of Oxford in 2014 . How it works . Code . Some imports . import os import sys import scipy.io import scipy.misc import matplotlib.pyplot as plt from matplotlib.pyplot import imshow from PIL import Image import numpy as np import tensorflow as tf from tensorflow.python.framework.ops import EagerTensor import pprint %matplotlib inline from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Set the style image . style_image = np.array(Image.open(&quot;/content/drive/MyDrive/Colab Notebooks/Tiles.jpg&quot;)) imshow(style_image) . . And the content image . content_image = np.array(Image.open(&quot;/content/drive/MyDrive/Colab Notebooks/boat.jpg&quot;)) print(np.shape(content_image)) imshow(content_image) . . Resize the images . img_size = 1100 # np.shape(content_image) content_image = Image.fromarray(content_image) content_image =np.array(content_image.resize((img_size, img_size))) content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape))) . style_image = Image.fromarray(style_image) style_image = np.array(style_image.resize((img_size, img_size))) style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape))) . Load parameters from the VGG model. A pretrained model for image classification . https://www.robots.ox.ac.uk/~vgg/research/very_deep/ . https://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md . tf.random.set_seed(272) pp = pprint.PrettyPrinter(indent=4) vgg = tf.keras.applications.VGG19(include_top=False, input_shape=(img_size, img_size, 3), weights=&#39;/content/drive/MyDrive/Colab Notebooks/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;) vgg.trainable = False pp.pprint(vgg) . &lt;keras.engine.functional.Functional object at 0x7f84e154af90&gt; . Now choose layers to represent the style of the image and assign style costs: Lower number more basic features . STYLE_LAYERS = [ (&#39;block1_conv1&#39;, .2), (&#39;block2_conv1&#39;, .2), (&#39;block3_conv1&#39;, .2), (&#39;block4_conv1&#39;, .2), (&#39;block5_conv1&#39;, .2)] . Compute the &quot;content cost&quot; using TensorFlow. . def compute_content_cost(content_output, generated_output): &quot;&quot;&quot; Computes the content cost Arguments: a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G Returns: J_content -- scalar that you compute using equation 1 above. &quot;&quot;&quot; a_C = content_output[-1] a_G = generated_output[-1] # Retrieve dimensions from a_G m, n_H, n_W, n_C = a_G.get_shape().as_list() # Reshape a_C and a_G a_C_unrolled = tf.reshape(a_C, shape=[m, n_H * n_W, n_C]) a_G_unrolled = tf.reshape(a_G, shape=[m, n_H * n_W, n_C]) # compute the cost with tensorflow J_content = (1/(4*n_H*n_W*n_C) )*tf.reduce_sum(tf.square( tf.subtract(a_C_unrolled, a_G_unrolled ) )) return J_content . the gram matrix of A is ùê∫ùê¥=ùê¥ùê¥ùëá. . def gram_matrix(A): &quot;&quot;&quot; Argument: A -- matrix of shape (n_C, n_H*n_W) Returns: GA -- Gram matrix of A, of shape (n_C, n_C) &quot;&quot;&quot; GA = tf.linalg.matmul( A, A, transpose_b=True) return GA . Compute the style cost for a single layer. . def compute_layer_style_cost(a_S, a_G): &quot;&quot;&quot; Arguments: a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G Returns: J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2) &quot;&quot;&quot; # Retrieve dimensions from a_G m, n_H, n_W, n_C = a_G.get_shape().as_list() # Reshape the images from (n_H * n_W, n_C) to have them of shape (n_C, n_H * n_W) a_S = tf.transpose(a_S) a_S=tf.reshape(a_S, shape=[n_C, n_H * n_W]) a_G = tf.transpose(a_G)#, shape=[n_C, n_H * n_W]) a_G=tf.reshape(a_G, shape=[n_C, n_H * n_W]) print(np.shape(a_S)) # Computing gram_matrices for both images S and G GS = gram_matrix(a_S) GG = gram_matrix(a_G) # Computing the loss (‚âà1 line) J_style_layer = J_content = (1/(4*(n_H*n_W)**2*n_C**2) )*tf.reduce_sum(tf.square( tf.subtract(GS, GG ) )) return J_style_layer . Compute style cost function, Calls individual layers cost funcxtion and applies a weight based on variable STYLE_LAYERS . def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS): &quot;&quot;&quot; Computes the overall style cost from several chosen layers Arguments: style_image_output -- our tensorflow model generated_image_output -- STYLE_LAYERS -- A python list containing: - the names of the layers we would like to extract style from - a coefficient for each of them Returns: J_style -- tensor representing a scalar value, style cost defined above by equation (2) &quot;&quot;&quot; # initialize the overall style cost J_style = 0 # Set a_S to be the hidden layer activation from the layer we have selected. # The last element of the array contains the content layer image, which must not to be used. a_S = style_image_output[:-1] # Set a_G to be the output of the choosen hidden layers. # The last element of the array contains the content layer image, which must not to be used. a_G = generated_image_output[:-1] for i, weight in zip(range(len(a_S)), STYLE_LAYERS): # Compute style_cost for the current layer J_style_layer = compute_layer_style_cost(a_S[i], a_G[i]) # Add weight * J_style_layer of this layer to overall style cost J_style += weight[1] * J_style_layer return J_style . A total cost function including both style and content costs . @tf.function() def total_cost(J_content, J_style, alpha = 10, beta = 40): &quot;&quot;&quot; Computes the total cost function Arguments: J_content -- content cost coded above J_style -- style cost coded above alpha -- hyperparameter weighting the importance of the content cost beta -- hyperparameter weighting the importance of the style cost Returns: J -- total cost as defined by the formula above. &quot;&quot;&quot; J = alpha*J_content +beta*J_style return J . . 5.3 Randomly Initialize the Image to be Generated . Now, you get to initialize the &quot;generated&quot; image as a noisy image created from the content_image. . The generated image is slightly correlated with the content image. | By initializing the pixels of the generated image to be mostly noise but slightly correlated with the content image, this will help the content of the &quot;generated&quot; image more rapidly match the content of the &quot;content&quot; image. | . generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32)) # noise = tf.random.uniform(tf.shape(generated_image), 0, 0.5) # generated_image = tf.add(generated_image, noise) generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0) . define a function which loads the VGG19 model and returns a list of the outputs for the middle layers. . def get_layer_outputs(vgg, layer_names): &quot;&quot;&quot; Creates a vgg model that returns a list of intermediate output values.&quot;&quot;&quot; outputs = [vgg.get_layer(layer[0]).output for layer in layer_names] model = tf.keras.Model([vgg.input], outputs) return model . Now, define the content layer and build the model. . content_layer = [(&#39;block5_conv4&#39;, 1)] vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer) . Save the outputs for the content and style layers in separate variables. . content_target = vgg_model_outputs(content_image) # Content encoder style_targets = vgg_model_outputs(style_image) # Style enconder . # Set a_C to be the hidden layer activation from the layer we have selected preprocessed_content = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32)) a_C = vgg_model_outputs(preprocessed_content) # Set a_G to be the hidden layer activation from same layer. Here, a_G references model[&#39;conv4_2&#39;] # and isn&#39;t evaluated yet. Later in the code, we&#39;ll assign the image G as the model input. a_G = vgg_model_outputs(generated_image) # Compute the content cost J_content = compute_content_cost(a_C, a_G) print(J_content) . tf.Tensor(0.0, shape=(), dtype=float32) . sets a_S to be the tensor giving the hidden layer activation for STYLE_LAYERS. . preprocessed_style = tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32)) a_S = vgg_model_outputs(preprocessed_style) # Compute the style cost J_style = compute_style_cost(a_S, a_G) print(J_style) . (64, 1210000) (128, 302500) (256, 75625) (512, 18769) (512, 4624) tf.Tensor(2067.7974, shape=(), dtype=float32) . Utils that you will need to display the images generated by the style transfer model. . def clip_0_1(image): &quot;&quot;&quot; Truncate all the pixels in the tensor to be between 0 and 1 Arguments: image -- Tensor J_style -- style cost coded above Returns: Tensor &quot;&quot;&quot; return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0) def tensor_to_image(tensor): &quot;&quot;&quot; Converts the given tensor into a PIL image Arguments: tensor -- Tensor Returns: Image: A PIL image &quot;&quot;&quot; tensor = tensor * 255 tensor = np.array(tensor, dtype=np.uint8) if np.ndim(tensor) &gt; 3: assert tensor.shape[0] == 1 tensor = tensor[0] return Image.fromarray(tensor) . Train a step . learning rate lower slower . optimizer = tf.keras.optimizers.Adam(learning_rate=0.03) @tf.function() def train_step(generated_image): with tf.GradientTape() as tape: # In this function you must use the precomputed encoded images a_S and a_C # Compute a_G as the vgg_model_outputs for the current generated image a_G = vgg_model_outputs(generated_image) # Compute the style cost J_style = compute_style_cost(a_S, a_G) # Compute the content cost J_content = compute_content_cost(a_C, a_G) # Compute the total cost J = total_cost(J_content, J_style, alpha = 10, beta = 40) grad = tape.gradient(J, generated_image) optimizer.apply_gradients([(grad, generated_image)]) generated_image.assign(clip_0_1(generated_image)) return J . Train the Model . # Uncoment to reset the style transfer process. You will need to compile the train_step function again generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32)) epochs = 5001 for i in range(epochs): train_step(generated_image) if i % 100 == 0: print(f&quot;Epoch {i} &quot;) if i % 100 == 0: image = tensor_to_image(generated_image) imshow(image) image.save(f&quot;image_{i}.jpg&quot;) . Some Examples . . .",
            "url": "https://thomashsimm.com/tensorflow/deep%20learning/jupyter/2021/11/17/StyleTransfer.html",
            "relUrl": "/tensorflow/deep%20learning/jupyter/2021/11/17/StyleTransfer.html",
            "date": " ‚Ä¢ Nov 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How Does a Bike-Share Navigate Speedy Success",
            "content": ". Introduction . Some quick analysis on Cycle bike-share data from Chicago . Welcome to the Cyclistic bike-share analysis case study! In this case study, you will perform many real-world tasks of a junior data analyst. You will work for a fictional company, Cyclistic, and meet different characters and team members. In order to answer the key business questions, you will follow the steps of the data analysis process: ask, prepare, process, analyze, share, and act. Along the way, the Case Study Roadmap tables ‚Äî including guiding questions and key tasks ‚Äî will help you stay on the right path. By the end of this lesson, you will have a portfolio-ready case study. Download the packet and reference the details of this case study anytime. Then, when you begin your job hunt, your case study will be a tangible way to demonstrate your knowledge and skills to potential employers. . Some imports . import pandas as pd from bs4 import BeautifulSoup import numpy as np import requests # from datetime import datetime, timezone import os import geopandas as gpd #get the current working directory owd=os.getcwd() . A function to extract web zip files and save to /dat folder . We&#39;ll just look at Q1 for 2020 data . The data for 2020 is split into Q1 and the rest is for each month. So we need to download and convert each to a data frame and then combine them together . def extractStuff(url): import requests, zipfile, io import os owd=os.getcwd() r = requests.get(url) z = zipfile.ZipFile(io.BytesIO(r.content)) z.extractall(owd+&quot;/dat/&quot;) URL=&quot;https://divvy-tripdata.s3.amazonaws.com/&quot; noma =[[&#39;202004-divvy-tripdata&#39;], [&#39;202005-divvy-tripdata&#39;], [&#39;202006-divvy-tripdata&#39;], [&#39;202007-divvy-tripdata&#39;], [&#39;202008-divvy-tripdata&#39;], [&#39;202009-divvy-tripdata&#39;], [&#39;202010-divvy-tripdata&#39;], [&#39;202011-divvy-tripdata&#39;], [&#39;202012-divvy-tripdata&#39;]] . for nom in noma: extractStuff(URL+ nom[0]+&quot;.zip&quot;) . Load files, put in pandas data frame and have a look . Lets load each csv and combine them . for i,nom in enumerate(noma): if i&gt;0: df=pd.read_csv(owd+&quot;/dat/&quot;+nom[0]+&#39;.csv&#39;) dfAll=pd.concat([df,dfAll]) print(&#39;1 &#39;,nom[0]) else: dfAll=pd.read_csv(owd+&quot;/dat/&quot;+nom[0]+&#39;.csv&#39;) print(&#39;2&#39;,nom[0]) . 2 202004-divvy-tripdata 1 202005-divvy-tripdata 1 202006-divvy-tripdata 1 202007-divvy-tripdata 1 202008-divvy-tripdata 1 202009-divvy-tripdata 1 202010-divvy-tripdata 1 202011-divvy-tripdata 1 202012-divvy-tripdata . How many NaN stations? . print(&#39;The percentage start stations NaN = {}&#39;.format(100*np.shape(dfAll[dfAll[&#39;start_station_id&#39;].isna()])[0] / np.shape(dfAll)[0]) )#95 282 3 114 796 print(&#39;The percentage end stations NaN = {}&#39;.format(100*np.shape(dfAll[dfAll[&#39;end_station_id&#39;].isna()])[0] / np.shape(dfAll)[0]) )#95 282 3 114 796 bothNa=dfAll[dfAll[&#39;start_station_id&#39;].isna() | dfAll[&#39;end_station_id&#39;].isna()] print(&#39;The percentage start stations NaN = {}&#39;.format(100*np.shape(bothNa)[0] / np.shape(dfAll)[0]) )#95 282 3 114 796 . The percentage start stations NaN = 3.059012532441932 The percentage end stations NaN = 3.5745840177013197 The percentage start stations NaN = 4.889597906251324 . import copy dfUse=copy.copy(dfAll[dfAll[&#39;start_station_id&#39;].notnull() &amp; dfAll[&#39;end_station_id&#39;].notnull()]) dfUse.describe(include=&#39;all&#39;) . ride_id rideable_type started_at ended_at start_station_name start_station_id end_station_name end_station_id start_lat start_lng end_lat end_lng member_casual . count 2962495 | 2962495 | 2962495 | 2962495 | 2962495 | 2962495.0 | 2962495 | 2962495.0 | 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | 2962495 | . unique 2962287 | 3 | 2568216 | 2556286 | 689 | 1301.0 | 690 | 1304.0 | NaN | NaN | NaN | NaN | 2 | . top 0A2B0949201A9D0C | docked_bike | 2020-09-07 15:19:26 | 2020-10-14 07:23:00 | Streeter Dr &amp; Grand Ave | 35.0 | Streeter Dr &amp; Grand Ave | 35.0 | NaN | NaN | NaN | NaN | member | . freq 2 | 2535257 | 12 | 13 | 32629 | 32192.0 | 34905 | 34467.0 | NaN | NaN | NaN | NaN | 1710201 | . mean NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.190534e+01 | -8.764452e+01 | 4.190559e+01 | -8.764481e+01 | NaN | . std NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.157203e-02 | 2.446731e-02 | 4.169299e-02 | 2.460637e-02 | NaN | . min NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.164850e+01 | -8.777470e+01 | 4.164850e+01 | -8.777470e+01 | NaN | . 25% NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.188316e+01 | -8.765840e+01 | 4.188338e+01 | -8.765862e+01 | NaN | . 50% NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.190096e+01 | -8.764117e+01 | 4.190096e+01 | -8.764182e+01 | NaN | . 75% NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.193120e+01 | -8.762773e+01 | 4.193125e+01 | -8.762775e+01 | NaN | . max NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 4.206490e+01 | -8.752823e+01 | 4.206501e+01 | -8.752823e+01 | NaN | . Now we need to convert the dates from object (i.e. string) to date format . next add a new column as time for hire in hours . dfUse.loc[:,&#39;started_at&#39;]=pd.to_datetime(dfUse[&#39;started_at&#39;],infer_datetime_format=True) dfUse.loc[:,&#39;ended_at&#39;]=pd.to_datetime(dfUse[&#39;ended_at&#39;],infer_datetime_format=True) delta=dfUse.iloc[:,3]-dfUse.iloc[:,2] dd=delta.dt.total_seconds()/(60*60) dfUse.insert(2,&quot;hire_time_h&quot;,dd) dfUse.head() . ride_id rideable_type hire_time_h started_at ended_at start_station_name start_station_id end_station_name end_station_id start_lat start_lng end_lat end_lng member_casual . 0 70B6A9A437D4C30D | classic_bike | 0.176944 | 2020-12-27 12:44:29 | 2020-12-27 12:55:06 | Aberdeen St &amp; Jackson Blvd | 13157 | Desplaines St &amp; Kinzie St | TA1306000003 | 41.877726 | -87.654787 | 41.888716 | -87.644448 | member | . 39 15F369FDAED4E8E3 | electric_bike | 0.130556 | 2020-12-18 13:53:56 | 2020-12-18 14:01:46 | Larrabee St &amp; Armitage Ave | TA1309000006 | Wells St &amp; Walton St | TA1306000011 | 41.918112 | -87.643799 | 41.900129 | -87.634448 | member | . 50 0CFD61DFE00E6043 | electric_bike | 0.030000 | 2020-12-28 17:10:25 | 2020-12-28 17:12:13 | Kingsbury St &amp; Kinzie St | KA1503000043 | Desplaines St &amp; Kinzie St | TA1306000003 | 41.889193 | -87.638576 | 41.889099 | -87.642479 | member | . 87 244CB936487039B7 | docked_bike | 1.013056 | 2020-12-10 13:36:16 | 2020-12-10 14:37:03 | Clark St &amp; Leland Ave | TA1309000014 | Clark St &amp; Leland Ave | TA1309000014 | 41.967096 | -87.667429 | 41.967096 | -87.667429 | casual | . 88 B7AD5038F79637F9 | classic_bike | 0.101111 | 2020-12-20 13:09:04 | 2020-12-20 13:15:08 | Dearborn St &amp; Monroe St | TA1305000006 | Kingsbury St &amp; Kinzie St | KA1503000043 | 41.881320 | -87.629521 | 41.889177 | -87.638506 | member | . Maybe we want the day of the week? . The day of the week with Monday=0, Sunday=6. . dfUse.insert(3,&#39;day_week&#39;,dfUse.loc[:,&#39;started_at&#39;].dt.dayofweek) dfUse.head() . ride_id rideable_type hire_time_h day_week started_at ended_at start_station_name start_station_id end_station_name end_station_id start_lat start_lng end_lat end_lng member_casual . 0 70B6A9A437D4C30D | classic_bike | 0.176944 | 6 | 2020-12-27 12:44:29 | 2020-12-27 12:55:06 | Aberdeen St &amp; Jackson Blvd | 13157 | Desplaines St &amp; Kinzie St | TA1306000003 | 41.877726 | -87.654787 | 41.888716 | -87.644448 | member | . 39 15F369FDAED4E8E3 | electric_bike | 0.130556 | 4 | 2020-12-18 13:53:56 | 2020-12-18 14:01:46 | Larrabee St &amp; Armitage Ave | TA1309000006 | Wells St &amp; Walton St | TA1306000011 | 41.918112 | -87.643799 | 41.900129 | -87.634448 | member | . 50 0CFD61DFE00E6043 | electric_bike | 0.030000 | 0 | 2020-12-28 17:10:25 | 2020-12-28 17:12:13 | Kingsbury St &amp; Kinzie St | KA1503000043 | Desplaines St &amp; Kinzie St | TA1306000003 | 41.889193 | -87.638576 | 41.889099 | -87.642479 | member | . 87 244CB936487039B7 | docked_bike | 1.013056 | 3 | 2020-12-10 13:36:16 | 2020-12-10 14:37:03 | Clark St &amp; Leland Ave | TA1309000014 | Clark St &amp; Leland Ave | TA1309000014 | 41.967096 | -87.667429 | 41.967096 | -87.667429 | casual | . 88 B7AD5038F79637F9 | classic_bike | 0.101111 | 6 | 2020-12-20 13:09:04 | 2020-12-20 13:15:08 | Dearborn St &amp; Monroe St | TA1305000006 | Kingsbury St &amp; Kinzie St | KA1503000043 | 41.881320 | -87.629521 | 41.889177 | -87.638506 | member | . Lets also get the time on its own . dfUse.insert(4,&#39;time_day&#39;,dfUse.loc[:,&#39;started_at&#39;].dt.hour + dfUse.loc[:,&#39;started_at&#39;].dt.minute/60) dfUse.head() . ride_id rideable_type hire_time_h day_week time_day started_at ended_at start_station_name start_station_id end_station_name end_station_id start_lat distance start_lng end_lat end_lng member_casual . 0 70B6A9A437D4C30D | classic_bike | 0.176944 | 6 | 12.733333 | 2020-12-27 12:44:29 | 2020-12-27 12:55:06 | Aberdeen St &amp; Jackson Blvd | 13157 | Desplaines St &amp; Kinzie St | TA1306000003 | 41.877726 | 1.491984 | -87.654787 | 41.888716 | -87.644448 | member | . 39 15F369FDAED4E8E3 | electric_bike | 0.130556 | 4 | 13.883333 | 2020-12-18 13:53:56 | 2020-12-18 14:01:46 | Larrabee St &amp; Armitage Ave | TA1309000006 | Wells St &amp; Walton St | TA1306000011 | 41.918112 | 2.144117 | -87.643799 | 41.900129 | -87.634448 | member | . 50 0CFD61DFE00E6043 | electric_bike | 0.030000 | 0 | 17.166667 | 2020-12-28 17:10:25 | 2020-12-28 17:12:13 | Kingsbury St &amp; Kinzie St | KA1503000043 | Desplaines St &amp; Kinzie St | TA1306000003 | 41.889193 | 0.323238 | -87.638576 | 41.889099 | -87.642479 | member | . 87 244CB936487039B7 | docked_bike | 1.013056 | 3 | 13.600000 | 2020-12-10 13:36:16 | 2020-12-10 14:37:03 | Clark St &amp; Leland Ave | TA1309000014 | Clark St &amp; Leland Ave | TA1309000014 | 41.967096 | 0.000000 | -87.667429 | 41.967096 | -87.667429 | casual | . 88 B7AD5038F79637F9 | classic_bike | 0.101111 | 6 | 13.150000 | 2020-12-20 13:09:04 | 2020-12-20 13:15:08 | Dearborn St &amp; Monroe St | TA1305000006 | Kingsbury St &amp; Kinzie St | KA1503000043 | 41.881320 | 1.147392 | -87.629521 | 41.889177 | -87.638506 | member | . And the distance travelled . def distanceLatLong(lat1,lon1,lat2,lon2): import numpy as np def deg2rad(deg): return deg * np.pi/180 R = 6371; # Radius of the earth in km dLat = deg2rad(lat2-lat1) # deg2rad below dLon = deg2rad(lon2-lon1) a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(deg2rad(lat1)) * np.cos(deg2rad(lat2)) * np.sin(dLon/2) * np.sin(dLon/2) c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a)) d = R * c ## Distance in km return d . d=distanceLatLong(dfUse[&quot;start_lat&quot;].values,dfUse[&quot;start_lng&quot;].values,dfUse[&quot;end_lat&quot;].values,dfUse[&quot;end_lng&quot;].values) dfUse.insert(11,&#39;distance&#39;,d) dfUse.describe() . hire_time_h day_week start_lat distance start_lng end_lat end_lng . count 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | 2.962495e+06 | . mean 4.291666e-01 | 3.262215e+00 | 4.190534e+01 | 2.240406e+00 | -8.764452e+01 | 4.190559e+01 | -8.764481e+01 | . std 6.640800e+00 | 1.973614e+00 | 4.157203e-02 | 2.029178e+00 | 2.446731e-02 | 4.169299e-02 | 2.460637e-02 | . min -4.841661e+02 | 0.000000e+00 | 4.164850e+01 | 0.000000e+00 | -8.777470e+01 | 4.164850e+01 | -8.777470e+01 | . 25% 1.377778e-01 | 2.000000e+00 | 4.188316e+01 | 8.598642e-01 | -8.765840e+01 | 4.188338e+01 | -8.765862e+01 | . 50% 2.525000e-01 | 3.000000e+00 | 4.190096e+01 | 1.713410e+00 | -8.764117e+01 | 4.190096e+01 | -8.764182e+01 | . 75% 4.577778e-01 | 5.000000e+00 | 4.193120e+01 | 3.098683e+00 | -8.762773e+01 | 4.193125e+01 | -8.762775e+01 | . max 9.786672e+02 | 6.000000e+00 | 4.206490e+01 | 4.837080e+01 | -8.752823e+01 | 4.206501e+01 | -8.752823e+01 | . Some issues arose above . hire_time_h max and min values | . Looks like the error is there from the start, so lets delete them . do the same for long times . dfUse=dfUse[dfUse.hire_time_h&gt;=0] dfUse=dfUse[dfUse.hire_time_h&lt;24] . Lets drop some columns for space . dfUse.drop(columns=[&quot;ride_id&quot;, &quot;started_at&quot;,&quot;ended_at&quot;,&quot;start_station_name&quot;,&quot;end_station_name&quot;],inplace=True) #,&quot;start_station_id&quot;,&quot;end_station_id&quot;] . save . dfUse.to_csv(&#39;/data/df_2020.csv&#39;) df=copy.copy(dfUse) . df = pd.read_csv(&#39;/data/df_2020.csv&#39;) df . C: Users 44781 anaconda3 lib site-packages IPython core interactiveshell.py:3165: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False. has_raised = await self.run_ast_nodes(code_ast.body, cell_name, . Unnamed: 0 rideable_type hire_time_h day_week time_day start_station_id end_station_id start_lat distance start_lng end_lat end_lng member_casual . 0 0 | classic_bike | 0.176944 | 6 | 12.733333 | 13157 | TA1306000003 | 41.877726 | 1.491984 | -87.654787 | 41.888716 | -87.644448 | member | . 1 39 | electric_bike | 0.130556 | 4 | 13.883333 | TA1309000006 | TA1306000011 | 41.918112 | 2.144117 | -87.643799 | 41.900129 | -87.634448 | member | . 2 50 | electric_bike | 0.030000 | 0 | 17.166667 | KA1503000043 | TA1306000003 | 41.889193 | 0.323238 | -87.638576 | 41.889099 | -87.642479 | member | . 3 87 | docked_bike | 1.013056 | 3 | 13.600000 | TA1309000014 | TA1309000014 | 41.967096 | 0.000000 | -87.667429 | 41.967096 | -87.667429 | casual | . 4 88 | classic_bike | 0.101111 | 6 | 13.150000 | TA1305000006 | KA1503000043 | 41.881320 | 1.147392 | -87.629521 | 41.889177 | -87.638506 | member | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2949984 84771 | docked_bike | 0.215278 | 3 | 16.166667 | 140.0 | 140.0 | 41.899000 | 0.000000 | -87.629900 | 41.899000 | -87.629900 | member | . 2949985 84772 | docked_bike | 0.319167 | 3 | 17.933333 | 322.0 | 351.0 | 41.799600 | 1.056377 | -87.594700 | 41.803000 | -87.606600 | casual | . 2949986 84773 | docked_bike | 1.886111 | 4 | 19.950000 | 236.0 | 182.0 | 41.907600 | 0.604983 | -87.638600 | 41.903200 | -87.634300 | casual | . 2949987 84774 | docked_bike | 0.708611 | 3 | 17.983333 | 310.0 | 310.0 | 41.920100 | 0.000000 | -87.677900 | 41.920100 | -87.677900 | casual | . 2949988 84775 | docked_bike | 0.100000 | 5 | 1.516667 | 138.0 | 138.0 | 41.904600 | 0.000000 | -87.640600 | 41.904600 | -87.640600 | casual | . 2949989 rows √ó 13 columns . Put frequency location onto a map . import folium from folium import plugins from folium.plugins import HeatMap lat=df[&#39;start_lat&#39;].values lon=df[&#39;start_lng&#39;].values latlon = [lat, lon] maps = folium.Map(location=[lat[0],lon[0]], zoom_start = 11) latlon=np.transpose(latlon) # Plot it on the map HeatMap(latlon).add_to(maps) # Display the map maps . . Bit of a mess, grouping by region may be better . Plot some choroplots . open and modify the geojson file- seems to make life easier later . import geopandas as gpd fname=&#39;Chicago.geojson&#39; chicago = gpd.read_file(fname) # neighborhoods aren&#39;t unique so lets use the index and call it ID chicago.reset_index(inplace=True) chicago.rename(columns={&#39;index&#39;:&#39;ID&#39;},inplace=True) chicago.drop(columns=[&#39;sec_neigh&#39;,&#39;shape_area&#39;,&#39;shape_len&#39;],inplace=True) chicago.to_file(&quot;Chi_.json&quot;, driver=&quot;GeoJSON&quot;) chicago.head() . ID pri_neigh geometry . 0 0 | Grand Boulevard | MULTIPOLYGON (((-87.60671 41.81681, -87.60670 ... | . 1 1 | Printers Row | MULTIPOLYGON (((-87.62761 41.87437, -87.62760 ... | . 2 2 | United Center | MULTIPOLYGON (((-87.66707 41.88885, -87.66707 ... | . 3 3 | Sheffield &amp; DePaul | MULTIPOLYGON (((-87.65833 41.92166, -87.65835 ... | . 4 4 | Humboldt Park | MULTIPOLYGON (((-87.74060 41.88782, -87.74060 ... | . Now we want to convert each station to a region within the json file . First let&#39;s create a variable for each station, with location and station_id . dfStat=df.groupby(by=[&#39;start_station_id&#39;]).mean() dfStat=dfStat.drop(columns=[&#39;Unnamed: 0&#39;,&#39;hire_time_h&#39;,&#39;day_week&#39;,&#39;time_day&#39;,&#39;distance&#39;,&#39;end_lat&#39;,&#39;end_lng&#39;]) dfStat.reset_index(inplace=True) dfStat . start_station_id start_lat start_lng . 0 2.0 | 41.876505 | -87.620535 | . 1 3.0 | 41.867228 | -87.615357 | . 2 4.0 | 41.856268 | -87.613345 | . 3 5.0 | 41.874050 | -87.627709 | . 4 6.0 | 41.886974 | -87.612813 | . ... ... | ... | ... | . 1877 TA1309000066 | 41.969101 | -87.674234 | . 1878 TA1309000067 | 41.803034 | -87.606613 | . 1879 WL-008 | 41.867122 | -87.641071 | . 1880 WL-011 | 41.880395 | -87.642727 | . 1881 WL-012 | 41.883364 | -87.641203 | . 1882 rows √ó 3 columns . Now for each station we want a JSON-area code . This is slightly convoluted . scroll through each station | for each station find if it&#39;s inside a Chigao_JSON region | if not we assign if a value 1000 | for those with no region find the nearest station that has a JSON-region (done in tab after this) | . whatChoro = json ID whatwhat = station ID . from shapely.geometry import shape, Point whatChoro=[] whatwhat=[] # check each polygon to see if it contains the point i=0 # scroll through each station for istat in range(np.shape(dfStat)[0]): i=0 #create a point for the station point=Point(dfStat.loc[istat,&#39;start_lng&#39;],dfStat.loc[istat,&#39;start_lat&#39;]) #scroll through each geometery for feature in chicago.ID: polygon = shape(chicago.loc[i,&#39;geometry&#39;]) if polygon.contains(point): #this gives the json region ID whatChoro.append(chicago.loc[i,&#39;ID&#39;]) #this give the station id whatwhat.append(dfStat.loc[istat,&#39;start_station_id&#39;]) break #if we don&#39;t get a match!! if feature==chicago[&#39;ID&#39;].iloc[-1]: import copy # find distances lat2/lng2 this location # lat1/lng1 all locations lat1_=copy.copy(dfStat[&#39;start_lat&#39;]) lon1_=copy.copy(dfStat[&#39;start_lng&#39;]) lat2_=dfStat.loc[istat,&#39;start_lat&#39;] lon2_=dfStat.loc[istat,&#39;start_lng&#39;] #this gives the json region ID whatChoro.append(1000) #this give the station id whatwhat.append(dfStat.loc[istat,&#39;start_station_id&#39;]) i=i+1 . This does the cleaning up if they don&#39;t have a json id . This will handle when we don&#39;t get a match&gt; a reuse of the distance function with slight mods . def distanceLatLong_v2(lat1_,lon1_,lat2_,lon2_): import numpy as np import math def deg2rad(deg): return deg * np.pi/180 def inner(lat1,lon1,lat2,lon2): R = 6371; # Radius of the earth in km dLat = deg2rad(lat2-lat1) # deg2rad below dLon = deg2rad(lon2-lon1) a = np.sin(dLat/2) * np.sin(dLat/2) + np.cos(deg2rad(lat1)) * np.cos(deg2rad(lat2)) * np.sin(dLon/2) * np.sin(dLon/2) c = 2 * math.atan2(np.sqrt(a), np.sqrt(1-a)) d = R * c ## Distance in km if d==0: d=1000 return d if np.shape(lat1_)[0]&gt;1: d=[] for i in range(np.shape(lat1_)[0]): d.append(inner(lat1_[i],lon1_[i],lat2_,lon2_)) else: d=inner(lat1_,lon1_,lat2_,lon2_) return d . this scrolls through ones we didn&#39;t match and finds nearest JSON-id we did match . for i in range(np.shape(dfStat)[0]): if whatChoro[i]==1000: #find distances lat2/lng2 this location # lat1/lng1 all locations lat1_=copy.copy(dfStat[&#39;start_lat&#39;]) lon1_=copy.copy(dfStat[&#39;start_lng&#39;]) lat2_=dfStat.loc[i,&#39;start_lat&#39;] lon2_=dfStat.loc[i,&#39;start_lng&#39;] ind=[idx for idx, element in enumerate(whatChoro) if element==1000] lat1_[ind]=0 lon1_[ind]=0 d=distanceLatLong_v2(lat1_,lon1_,lat2_,lon2_) indamin=d.index(min(d)) # whatwhat[i]=whatwhat[indamin] whatChoro[i]=whatChoro[indamin] print(i,indamin,whatChoro[i],whatChoro[indamin],min(d)) . 554 474 48 48 3.218616605981318 555 474 48 48 1.5436541433245226 556 474 48 48 0.7244340745224377 557 554 48 48 1.618989106320433 558 557 48 48 0.9589393666816851 559 558 48 48 1.703490590200107 560 559 48 48 1.0394756465095663 561 554 48 48 0.5321339843549502 562 561 48 48 0.8611490298165934 563 561 48 48 0.4535879672825213 570 554 48 48 0.7620712283526319 603 562 48 48 0.7657516801324228 604 1355 55 55 0.9398603094768803 605 559 48 48 1.3757808765327746 1441 554 48 48 0.0009191040374285816 1442 1441 48 48 0.003706303019684898 1443 555 48 48 0.006456249041029345 1444 556 48 48 4.9908562509422157e-05 1445 1444 48 48 0.002230404415936471 1446 557 48 48 0.0012372240336437155 1447 1446 48 48 0.003412919018808641 1450 558 48 48 0.005285184511373336 1451 1450 48 48 0.011408782497315317 1452 559 48 48 0.027327480291422585 1453 560 48 48 0.0012042612006966005 1454 561 48 48 0.0010429138707679156 1455 562 48 48 0.0017205826064828574 1456 1455 48 48 0.0034819520850311483 1457 563 48 48 0.0025804458419044335 1458 563 48 48 0.009202498604403178 1470 570 48 48 0.004764480272445732 1513 603 48 48 0.0008354495738933785 1514 603 48 48 0.0011940325501674988 1515 604 55 55 0.006060872681823562 1516 1515 55 55 0.012603279140811706 1517 605 48 48 0.009833621616663023 1582 555 48 48 0.0011726503764087032 1583 559 48 48 0.0038048094986119284 1584 1453 48 48 0.0005568291527966493 1585 561 48 48 0.0005786222457187065 1586 1447 48 48 0.5069880704727254 1587 605 48 48 0.0032438412454957833 . Now we can insert a new column in df with the json ID . chicID=[] for stat in df[&#39;start_station_id&#39;]: # chicID.append(stat) ind=[idx for idx, element in enumerate(whatwhat) if element==stat] try: chicID.append(whatChoro[ind[0]]) except: continue . df.insert(0,&#39;ID&#39;,chicID) df.head() . ID Unnamed: 0 rideable_type hire_time_h day_week time_day start_station_id end_station_id start_lat distance start_lng end_lat end_lng member_casual . 0 40 | 0 | classic_bike | 0.176944 | 6 | 12.733333 | 13157 | TA1306000003 | 41.877726 | 1.491984 | -87.654787 | 41.888716 | -87.644448 | member | . 1 92 | 39 | electric_bike | 0.130556 | 4 | 13.883333 | TA1309000006 | TA1306000011 | 41.918112 | 2.144117 | -87.643799 | 41.900129 | -87.634448 | member | . 2 97 | 50 | electric_bike | 0.030000 | 0 | 17.166667 | KA1503000043 | TA1306000003 | 41.889193 | 0.323238 | -87.638576 | 41.889099 | -87.642479 | member | . 3 56 | 87 | docked_bike | 1.013056 | 3 | 13.600000 | TA1309000014 | TA1309000014 | 41.967096 | 0.000000 | -87.667429 | 41.967096 | -87.667429 | casual | . 4 26 | 88 | classic_bike | 0.101111 | 6 | 13.150000 | TA1305000006 | KA1503000043 | 41.881320 | 1.147392 | -87.629521 | 41.889177 | -87.638506 | member | . And represent each JSON region by how many times they&#39;re used . We&#39;ll take the count and divide it by the total- and because of the distribution we&#39;ll also take the log- basically hires are highly focussed on a few regions with many having low % . dfG=df.groupby(&#39;ID&#39;).count() dfG.reset_index(inplace=True) dfG=dfG[[&#39;ID&#39;,&#39;rideable_type&#39;]] dfG.rename(columns={&#39;rideable_type&#39;:&#39;Frequency&#39;}) dfG.rideable_type=np.log(dfG.rideable_type/sum(dfG.rideable_type)) dfG.head() . ID rideable_type . 0 0 | -5.835124 | . 1 1 | -5.124845 | . 2 2 | -6.001956 | . 3 3 | -3.718336 | . 4 4 | -4.965188 | . df.to_csv(&#39;/data/dfChoro_2020.csv&#39;) . df = pd.read_csv(&#39;/data/dfChoro_2020.csv&#39;) . C: Users 44781 anaconda3 lib site-packages IPython core interactiveshell.py:3165: DtypeWarning: Columns (7,8) have mixed types.Specify dtype option on import or set low_memory=False. has_raised = await self.run_ast_nodes(code_ast.body, cell_name, . Now the plotting . We first read in the json file, add the df with our frequency values to it then we can plot the data . Lets put this in a function to look at differences . def bigChoro(dfIN,colname,choi): import folium LEGNOM=colname if choi==&#39;count&#39;: dfG=dfIN.groupby(&#39;ID&#39;).count() dfG[colname]=dfG[colname]/(100*274/7) myscale = (dfG[colname].quantile((0,0.25,0.5,0.75,0.9,0.95,.97,1))).tolist() # np.linspace(dfG[colname].min(),dfG[colname].max(),10) LEGNOM=&#39;Number of journeys 100s per week&#39; elif choi==&#39;mean&#39;: dfG=dfIN.groupby(&#39;ID&#39;).mean() myscale = np.linspace(dfG[colname].min(),dfG[colname].max(),10) elif choi==&#39;sum&#39;: dfG=dfIN.groupby(&#39;ID&#39;).sum() myscale = np.linspace(dfG[colname].min(),dfG[colname].max(),10) elif choi==&#39;dayofweek&#39;: dfIN=dfIN[[&#39;ID&#39;,colname]] dfG=dfIN.groupby([&#39;ID&#39;]).agg(lambda x:x.value_counts().index[0]) dfG[dfG[colname]&gt;4]=5-dfG[dfG[colname]&gt;4] myscale = np.array([-2.,0.,1.,2.,3.,4.]) LEGNOM=&#39;Day of week (-2 to -1 weekend, 0-4 Monday to Friday)&#39; elif choi==&#39;mode&#39;: dfIN=dfIN[[&#39;ID&#39;,colname]] dfIN[colname].astype(&#39;int32&#39;) dfG=dfIN.groupby([&#39;ID&#39;]).agg(lambda x:x.value_counts().index[0]) myscale = np.linspace(dfG[colname].min(),dfG[colname].max(),10) dfG.reset_index(inplace=True) dfG=dfG[[&#39;ID&#39;,colname]] nil=gpd.read_file(&quot;Chi_.json&quot;) nil=nil[[&#39;ID&#39;,&#39;geometry&#39;]] # merge data frames nilpop=nil.merge(dfG,on=&quot;ID&quot;) #initial map m = folium.Map(location=[41.884,-87.6247], zoom_start=10, control_scale=True,tiles=&quot;Stamen Toner&quot;)#,tiles = t_list[1]) folium.TileLayer(&#39;CartoDB positron&#39;,name=&quot;Light Map&quot;,control=False).add_to(m) # (dfG[&#39;rideable_type&#39;].quantile((0,.02,0.1,.25,0.5,0.75,0.9,0.95,0.98,1))).tolist() choropleth =folium.Choropleth( geo_data=&quot;Chi_.json&quot;, data=nilpop, threshold_scale=myscale, columns=[&#39;ID&#39;,colname], name=&#39;choropleth&#39;, fill_color=&#39;BuPu&#39;,#PuBuGn YlGn PuBuGn YlGnBu RdYlBu key_on= &quot;feature.properties.ID&quot;, fill_opacity=0.7, line_opacity=0.2, nan_fill_color=&#39;gray&#39;, legend_name=LEGNOM, nan_fill_opacity =.5, ).add_to(m) folium.LayerControl().add_to(m) choropleth.geojson.add_child( folium.features.GeoJsonTooltip([&#39;pri_neigh&#39;],labels=False) ) return m . dfIN.columns . Index([&#39;ID&#39;, &#39;Unnamed: 0&#39;, &#39;rideable_type&#39;, &#39;hire_time_h&#39;, &#39;day_week&#39;, &#39;time_day&#39;, &#39;start_station_id&#39;, &#39;end_station_id&#39;, &#39;start_lat&#39;, &#39;distance&#39;, &#39;start_lng&#39;, &#39;end_lat&#39;, &#39;end_lng&#39;, &#39;member_casual&#39;], dtype=&#39;object&#39;) . colname=&#39;distance&#39;#end_station_id dfIN= df[df.member_casual==&#39;member&#39;] m=bigChoro(dfIN,colname,&#39;mean&#39;) m . . colname=&#39;day_week&#39;#end_station_id dfIN= df[df.member_casual==&#39;member&#39;] m=bigChoro(dfIN,colname,&#39;dayofweek&#39;) m . . colname=&#39;time_day&#39;#end_station_id dfIN= df[df.member_casual==&#39;member&#39;] m=bigChoro(dfIN,colname,&#39;mode&#39;) m . . colname=&#39;time_day&#39;#end_station_id dfIN= df[df.member_casual==&#39;member&#39;] m=bigChoro(dfIN,colname,&#39;count&#39;) m . . colname=&#39;time_day&#39;#end_station_id dfIN= df[df.member_casual==&#39;casual&#39;] m=bigChoro(dfIN,colname,&#39;count&#39;) m . . np.linspace(-2,4,7) np.array([-2.,0.,1.,2.,3.,4.]) . array([-2., 0., 1., 2., 3., 4.]) . import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt . sns.set_theme(style=&quot;ticks&quot;) f, ax = plt.subplots(figsize=(7, 5)) sns.despine(f) # Draw a nested boxplot to show bills by day and time sns.histplot(df,hue=&quot;member_casual&quot;, x=&quot;time_day&quot;, multiple=&quot;stack&quot;, palette=&quot;dark:b_r&quot;, edgecolor=&quot;.3&quot;, linewidth=.5,) sns.despine(offset=10, trim=True) . from matplotlib.ticker import PercentFormatter df_=df[df.hire_time_h&lt;5] df_=df_[df_.hire_time_h&gt;0] sns.set_theme(style=&quot;ticks&quot;) binwidth = 5 f, ax1 = plt.subplots(figsize=(10, 7)) sns.despine(f) df2_=df_[df_.member_casual==&#39;member&#39;] # Draw a nested boxplot to show bills by day and time sns.histplot(df2_, x=&quot;hire_time_h&quot;, multiple=&quot;stack&quot;, palette=&quot;light:m_r&quot;, edgecolor=&quot;.3&quot;, linewidth=.5, stat=&#39;probability&#39;, log_scale=True, ax=ax1, label=&#39;member&#39;) ax2=ax1.twinx() df2_=df_[df_.member_casual==&#39;casual&#39;] # Draw a nested boxplot to show bills by day and time sns.histplot(df2_, x=&quot;hire_time_h&quot;, element=&quot;step&quot;,fill=False, color=&#39;red&#39;, linewidth=.8, stat=&#39;probability&#39;, log_scale=True, ax=ax2, label=&#39;casual&#39;) ax1.legend(loc=&#39;upper left&#39;) ax2.legend(loc=&#39;upper right&#39;) sns.despine(offset=10, trim=True) # ax.legend(&#39;Member&#39;,&#39;Casual&#39;) . sns.set_theme(style=&quot;ticks&quot;) f, ax = plt.subplots(figsize=(10, 8)) sns.despine(f) df_=df[ df[&quot;distance&quot;]&gt;0.1 ] df_= df_[df_[&quot;distance&quot;]&lt;20] df_=df_[df_.member_casual==&#39;member&#39;] # Draw a nested boxplot to show bills by day and time sns.histplot(data=df_, x=&quot;distance&quot;, edgecolor=&quot;.3&quot;, linewidth=.5, stat=&#39;probability&#39;, label=&#39;member&#39;) sns.despine(offset=10, trim=True) df_=df[df[&quot;distance&quot;]&gt;0.1] df_= df_[df_[&quot;distance&quot;]&lt;20] df_=df_[df_.member_casual==&#39;casual&#39;] sns.histplot(data=df_, x=&quot;distance&quot;, linewidth=.8, color=&#39;r&#39;, stat=&#39;probability&#39;, label=&#39;casual&#39;, fill=False, element=&#39;step&#39;) ax.legend(loc=&#39;upper right&#39;) . &lt;matplotlib.legend.Legend at 0x2f212cc0be0&gt; . sns.set_theme(style=&quot;whitegrid&quot;) # Draw a nested boxplot to show bills by day and time ax=sns.histplot(df,hue=&quot;member_casual&quot;, x=&quot;day_week&quot;,palette=&quot;dark:b_r&quot;, multiple=&quot;dodge&quot;, bins=[0 ,1 ,2 ,3, 4, 5, 6,7], shrink=.9 ) sns.despine(offset=20, trim=True) aa=np.array([0,1,2,3,4,5,6])+.5 ax.set_xticks(aa) ax.set_xlim([0, 7.5]) lab=[&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Wednesday&#39;,&#39;Thursday&#39;,&#39;Friday&#39;,&#39;Saturday&#39;,&#39;Sunday&#39;] ax.set_xticklabels(lab,rotation=&#39;vertical&#39;) ax.set_xlabel(&#39;Day of the week&#39;) . Text(0.5, 0, &#39;Day of the week&#39;) . So after a quick look at the data (*maybe some plots need mods) some clear trends: . Casuals use bikes more on weekends, members more on weekdays | Members tend to use bikes in commuting times 7-9 am and 4-7 pm. Whereas casuals more spread but focussed later | Casuals tend to use the bikes for longer and travel further from initial location | .",
            "url": "https://thomashsimm.com/2021/11/16/ChicagoBikes.html",
            "relUrl": "/2021/11/16/ChicagoBikes.html",
            "date": " ‚Ä¢ Nov 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Python Imbedding Web",
            "content": "Ipywidgets . https://ipywidgets.readthedocs.io/en/latest/ . ipywidgets, also known as jupyter-widgets or simply widgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel.&lt;/p&gt; Notebooks come alive when interactive widgets are used. Users gain control of their data and can visualize changes in the data. . Learning becomes an immersive, fun experience. Researchers can easily see how changing inputs to a model impact the results. &lt;/blockquote &gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Import . import ipywidgets as widgets . A slider . widgets.IntSlider() . Change the details of the slider . widgets.IntSlider( value=7, min=0, max=10, step=1, description=&#39;Test:&#39;, disabled=False, continuous_update=False, orientation=&#39;horizontal&#39;, readout=True, readout_format=&#39;d&#39; ) . Get the value . x=widgets.IntSlider() print(&#39;The value is {}&#39;.format(str(x.value))) x . The value is 0 . A different way with interact . from ipywidgets import interact #some function we want to call def square(x): return x * x *2 #the new widget slider interact(square, x=(0, 100, 10)); . A text box . widgets.Text(value=&#39;Hello World!&#39;, disabled=True) . A toggle button . widgets.ToggleButton( value=False, description=&#39;Click me&#39;, disabled=False, button_style=&#39;&#39;, # &#39;success&#39;, &#39;info&#39;, &#39;warning&#39;, &#39;danger&#39; or &#39;&#39; tooltip=&#39;Description&#39;, icon=&#39;check&#39; # (FontAwesome names without the `fa-` prefix) ) . A dropdown menu . widgets.Dropdown( options=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;], value=&#39;2&#39;, description=&#39;Number:&#39;, disabled=False, ) . A radiobutton with a label in a box . widgets.Box( [ widgets.Label(value=&#39;Pizza topping with a very long label:&#39;), widgets.RadioButtons( options=[ &#39;pepperoni&#39;, &#39;pineapple&#39;, &#39;anchovies&#39;, &#39;and the long name that will fit fine and the long name that will fit fine and the long name that will fit fine &#39; ], layout={&#39;width&#39;: &#39;max-content&#39;} ) ] ) . Some html . widgets.HTMLMath( value=r&quot;Some math and &lt;i&gt;HTML&lt;/i&gt;: (x^2 ) and $$ frac{x+1}{x-1}$$&quot;, placeholder=&#39;Some HTML&#39;, description=&#39;Some HTML&#39;, ) . Accordion, with slider and textbox . accordion = widgets.Accordion(children=[widgets.IntSlider(), widgets.Text()], titles=(&#39;Slider&#39;, &#39;Text&#39;)) accordion . multiple sliders . a = widgets.IntSlider(description=&#39;a&#39;) b = widgets.IntSlider(description=&#39;b&#39;) c = widgets.IntSlider(description=&#39;c&#39;) def f(a, b, c): print(&#39;{}*{}*{}={}&#39;.format(a, b, c, a*b*c)) out = widgets.interactive_output(f, {&#39;a&#39;: a, &#39;b&#39;: b, &#39;c&#39;: c}) widgets.HBox([widgets.VBox([a, b, c]), out]) . Inserting into a website . After installing nbinteract a html file of a python notebook can be created . nbinteract Quick_ipyWidget.ipynb -s thomashsimm/TestWdgets/main . &lt;/div&gt; .",
            "url": "https://thomashsimm.com/2021/10/29/Imbedding-Python-Web.html",
            "relUrl": "/2021/10/29/Imbedding-Python-Web.html",
            "date": " ‚Ä¢ Oct 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Data Viz",
            "content": "toc:true | branch: master | badges: true | comments: true | author: Thomas H. Simm | categories: [data viz, posters, data science, Thomas Simm] | . . Some data visualizations from my work in metallurgy. Most of my work was line plots so tried to keep these to a minimum here. . EBSD . Electron backscatter diffraction (EBSD) is a scanning electron microscope‚Äìbased microstructural-crystallographic characterization technique commonly used in the study of crystalline or polycrystalline materials.[1][2] The technique can provide information about the structure,[3] crystal orientation ,[3]phase,[3] or strain[4] in the material. . [1] Randle, Valerie; Engler, Olaf (2000). Introduction to texture analysis: macrotexture, microtexture and orientation mapping (Digital printing 2003 ed.). Boca Raton: CRC Press. ISBN 978-9056992248. . [2] Schwartz, A. J.; Kumar, M.; Adams, B. L.; Field, D. P. (2000). Electron backscatter diffraction in materials science. New York: Kluwer Academic. . [3] Electron backscatter diffraction in materials science (2nd ed.). Springer Science+Business Media. 2009. p. 1. ISBN 978-0-387-88135-5. . [4] Wright, Stuart I.; Matthew, M. Nowell; David, P. Field. (2011). &quot;A review of strain analysis using electron backscatter diffraction&quot;. Microscopy and Microanalysis. 17. 17 (3): 316‚Äì329. Bibcode:2011MiMic..17..316W. doi:10.1017/S1431927611000055. PMID 21418731. . From: https://en.wikipedia.org/wiki/Electron_backscatter_diffraction . . A matenistic steel showing prior austenite grains. The top image has undergone an additional rolling regime resulting in smaller grain size. This is your classic EBSD orientation map with a twist in that the orientations are predicted. The colours represent different orientations given by the legend on the left- they are a vector [phi1,Phi,phi2] (MTEX) with each spatial point having it‚Äôs own vector. The colours are reconstructed using ARPGE to give a prediction on what the grains would be prior to cooling, based on the rotation matrix between adjacent elements. There are two problems here, defining the orientation relationships (or rotations) between neighbours and reconstructing grains. In the next case we can ignore the first. . **Paper**: The Influence of Lath, Block and Prior Austenite Grain (PAG) Size on the Tensile, Creep and Fatigue Properties of Novel Maraging Steel . An austenitic stainless steel that transforms to martensite under load. This is similar to the above example, but easier to solve as we only need to look at the rotation across one boundary at a time. In the top figure the coloured regions represent a different phase (martensite) with austenite grains and the particular colour what the orientation relationship (OR) is. The lines show the slip systems and the maximum Schmid factor. Basically, some OR are preferred, the direction of the martensite relates to the particular OR and the Schmid factors. . **Paper**: In situ observation of strain and phase transformation in plastically deformed 301 austenitic stainless steel . The change in various EBSD maps of an austenitic stainless steel (see above) before and after being pulled to 10% strain. See also the image below. Each set of maps (on horizontal axis and below) give a different measure of plastic deformation. But if we look closely we can see some areas with high values of one parameter can have low values of another. Even if we average over a grain it can be difficult to predict behaviour. These observations illustrate aspects of plastic deformation such as the chaotic nature and the none unique definition of plastic deformation. . **Paper**: The œÑ-plot, a multicomponent 1-D pole figure plot, to quantify the heterogeneity of plastic deformation . This is a localised strain map of the sample shown above but a slightly bigger region. This is produced by digital image correlation (DIC) by comparing two surface images. The data is then combined with the EBSD data allowing us to visualise the grain boundaries (black lines). . So how do we take account of orientation differences in grains (they matter) AND the chaotic nature of deformation and differences in parameters? . We do some averageing . This is a classic plot in metallurgy called the inverse pole figure (IPF) plot. Simply put each point on the triangle represents a different group of orientations. We are averageing based on an orientation criteria. In the top figures are two models that relate to plastic deformation (Schmid factor left and Taylor model right) plotted on to this IPF plot. The bottom two figures represent experimental data of a ‚ÄòDamage parameter‚Äô (number of un-indexed points on boundary) of grains after creep deformation of an austenitic stainless steel. Clearly (hopefully), we can see a transition from type with increasing stress and overall plastic deformation. {Some adjustments could be done on the algorithm to create the plots but the main points remain} . . In a similar manner we can average details of a sample based on orientation in a different manner, as shown here. The reason for this averaging is so we can combine EBSD with powder diffraction (X-ray and neutron). Powder diffraction provides useful insights on some parameters connected to EBSD such as texture, plastic deformation and phases. Furthermore, the information can be from larger volumes of the sample (and not just the surface) than EBSD. However, the way it is measured means we have to modify how we combine the data. In the figure crystal plasticity models are compared with experimental data for EBSD and powder diffraction on the same axis. . **Paper**: The œÑ-plot, a multicomponent 1-D pole figure plot, to quantify the heterogeneity of plastic deformation . APT . The atom probe was introduced at the 14th Field Emission Symposium in 1967 by Erwin Wilhelm M√ºller and J. A. Panitz. It combined a field ion microscope with a mass spectrometer having a single particle detection capability and, for the first time, an instrument could ‚Äú‚Ä¶ determine the nature of one single atom seen on a metal surface and selected from neighboring atoms at the discretion of the observer‚Äù.[1] . https://en.wikipedia.org/wiki/Atom_probe . The image is an APT tip (a 3D cylinder type shape) showing iso-surfaces for two different elements (i.e. inside these surfaces the composition of an element {Ni and Mo here} is higher than a set value). This image is typical of APT analysis and produced from a designated software package. . This transforms data similar to shown above, first the APT data was cut into slices- this makes it easier to visualize the density and details and compares better with other techniques such as TEM. Secondly, I adjusted colours and contrast to improve the visual feel. . The above maps are pretty but very qualitative. Some extra details can be found by extracting details from the data as shown here. . Posters . . . . .",
            "url": "https://thomashsimm.com/2021/10/24/Data-Viz.html",
            "relUrl": "/2021/10/24/Data-Viz.html",
            "date": " ‚Ä¢ Oct 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Git and Github",
            "content": ". Introduction to Git and GitHub . From the Coursera course within Google IT Automation with Python Professional Certificate https://www.coursera.org/learn/introduction-git-github/home/welcome . Check out the following links for more information: . https://git-scm.com/doc . https://en.wikipedia.org/wiki/Version_control . The Linux kernel documentation itself (https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/process/submitting-patches.rst?id=HEAD), as well as impassioned opinions from other developers. (http://stopwritingramblingcommitmessages.com/ https://robots.thoughtbot.com/5-useful-tips-for-a-better-commit-message) . You can check out &quot;Setting your email in Git&quot; (https://help.github.com/articles/setting-your-email-in-git/) and &quot;Keeping your email address private&quot; on the GitHub help site for how to do this. (https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-email-preferences/setting-your-commit-email-address) . Run in github bash . Git is a distributed version control system. . Distributed means that each developer has a copy of the whole repository on their local machine. . . diff find the difference between two files . diff . https://man7.org/linux/man-pages/man1/diff.1.html . diff is used to find differences between two files. On its own, it‚Äôs a bit hard to use; instead, use it with diff -u to find lines which differ in two files: diff -u . diff -u is used to compare two files, line by line, and have the differing lines compared side-by-side in the same output. See below: . &gt;&gt;diff test.py test2.py . output . 3c3 &lt; c=a**b &gt; c=a**b+a 4a5 &gt; d=c**c . 3c3 . line in file 1 - c=change - line in file 2 a=added . &gt;&gt;diff -u test.py test2.py #output test.py 2021-10-17 11:14:24.880950400 +0100 +++ test2.py 2021-10-17 11:14:21.155278200 +0100 @@ -1,4 +1,5 @@ a=3 b=2 -c=a**b +c=a**b+a print(c) +d=c**c . adding -u gives extra context to changes . applying changes . Create a file with the changes in . &gt;&gt;diff -u test.py test2.py &gt;change.diff . patch . http://man7.org/linux/man-pages/man1/patch.1.html . &gt;&gt;patch test-Copy1.py &lt; change.diff . this applies changes in change.diff to test-Copy1.diff . VCS version control system . By keeping track of the changes that we make to our files, a VCS lets us know when a file changed, who changed it, and also lets us easily roll back those changes. . git config . &gt;&gt;git config --global user.email &quot;me@example.com&quot; &gt;&gt;git config --global user.name &quot;My name&quot; . set email and name . --global means for all repositories . &gt;&gt;git config -l . create a new repository . &gt;&gt;mkdir testa &gt;&gt;cd testa &gt;&gt;git init . detals of the created directory . &gt;&gt;ls -l .git #output-&gt;&gt; -rw-r--r-- 1 44781 197609 23 Oct 17 12:06 HEAD -rw-r--r-- 1 44781 197609 130 Oct 17 12:06 config -rw-r--r-- 1 44781 197609 73 Oct 17 12:06 description drwxr-xr-x 1 44781 197609 0 Oct 17 12:06 hooks/ drwxr-xr-x 1 44781 197609 0 Oct 17 12:06 info/ drwxr-xr-x 1 44781 197609 0 Oct 17 12:06 objects/ drwxr-xr-x 1 44781 197609 0 Oct 17 12:06 refs/ . The git directory acts as a database for all the changes tracked in Git and the working tree acts as a sandbox where we can edit the current versions of the files. . add a file to the directory and get it tracked . &gt;&gt;cp ../test.py . #note the dot at end to say copy to this directory &gt;&gt;git add test.py . The file is now in the staging area or index . contains all info about what files and changes are in the next commit . &gt;&gt;git add -p . this would all tracked and shows differences and asks if want to make the add . get the status . &gt;&gt;git status On branch master No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: test.py . test.py ready to be committed . Make a commit . &gt;&gt;git commit # Please enter the commit message for your changes. Lines starting # with &#39;#&#39; will be ignored, and an empty message aborts the commit. # # On branch master # # Initial commit # # Changes to be committed: # new file: test.py # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ . this opens up an editor with text as shown . Enter details of the commit . To exit and save: Type the description at the very top, press esc to exit insert mode, then type :x! (now the cursor is at the bottom) and hit enter to save and exit . Alternatively add -m command with details of commit . git commit -m &#39;Changed print to include some text&#39; . Tracked files . Modified made changes but not committed yet | . | Staged in staging area | git add | . | Committed in git directory | git commit | . | . Commit messages . Brief description . More indepth details . More indepth . (remember to add new lines) . Git log . &gt;&gt;git log commit 05958854b7ad68f8b1a178f260dccd844aec0e16 (HEAD -&gt; master) Author: thomashsimm &lt;thomas @gmail.com&gt; Date: Sun Oct 17 13:04:10 2021 +0100 added python start line commit 917a027f6282ce1c2c01e00a48bb72995e3984a6 Author: thomashsimm &lt;thomas @gmail.com&gt; Date: Sun Oct 17 12:56:41 2021 +0100 Changed print to include some text commit 7e32e328db17594fd0a507b4a23f489b68c09543 Author: thomashsimm &lt;thomas @gmail.com&gt; Date: Sun Oct 17 12:38:02 2021 +0100 Add new file test.py . Skipping the Staging Area . &gt;&gt;git commit -a or &gt;&gt;git commit -a -m &quot;Added a new output&quot; . a shortcut to stage any changes to tracked files and commit them in one step . Getting more information about our changes . &gt;&gt;git log -p . Equivalent to diff -u (describe above) . to get more info on a particular commit . &gt;&gt;git show 7e32e328db17594fd0a507b4a23f489b68c09543 . Some stats of repo . &gt;&gt;git log --stat . changes in file, relative to repo/staging area . &gt;&gt;git diff . Deleting and Renaming Files . &gt;&gt;git rm test.py . removes the file test.py . &gt;&gt;git commit -m &quot;Deleted test.py&quot; . commit the change/deletion of the file . &gt;&gt;git mv test2.py test.py . rename a file . N.B. git mv can also be used for moving files . &gt;&gt;echo .DS_STORE &gt; .gitignore #create .gitignore file to ignore .DS_STORE files (OS sys) &gt;&gt;ls -la #view hidden files &gt;&gt;git add .gitignore #add file &gt;&gt;git commit -m &quot;added a gitignore file&quot; #commit changes . Overview git cheat sheet . git commit -a &nbsp;&nbsp;&nbsp; Stages files automatically . git log -p &nbsp;&nbsp;&nbsp; Produces patch text . git show&nbsp;&nbsp;&nbsp;Shows various objects . git diff&nbsp;&nbsp;&nbsp;Is similar to the Linux diff command, and can show the differences in various commits . git diff --staged&nbsp;&nbsp;&nbsp;An alias to --cached, this will show all staged files compared to the named commit . git add -p &nbsp;&nbsp;&nbsp;Allows a user to interactively review patches to add to the current commit . git mv&nbsp;&nbsp;&nbsp;Similar to the Linux mv command, this moves a file . git rmSimilar to the Linux rm command, this deletes, or removes a file . There are many useful git cheatsheets online as well. Please take some time to research and study a few, such as this one. https://github.github.com/training-kit/downloads/github-git-cheat-sheet.pdf . .gitignore files . .gitignore files are used to tell the git tool to intentionally ignore some files in a given Git repository. For example, this can be useful for configuration files or metadata files that a user may not want to check into the master branch. Check out more at: https://git-scm.com/docs/gitignore. . A few common examples of file patterns to exclude can be found here https://gist.github.com/octocat/9257657. . Undoing changes . Undoing unstaged changes . &gt;&gt;git restore test.py . &quot;git restore ...&quot; to discard changes in working directory&lt;/p&gt; This comes up as an option to do when go git status . Same as git checkout . This takes the file from the directory and changes the one in the working tree to match this . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Undoing staged changes . Say we&#39;ve added all files in a directory . git add * . When we run git status we reaslise it includes a file we don&#39;t want to add . To remove the add we do: . &gt;&gt;git restore --staged newfile.txt or &gt;&gt;git reset HEAD newfile.txt . Amending commits . &gt;git commit --amend . This allows us to ammend the last commit . So for example we could add some more files to it and then modify the comments on the commit . Then overwrites the previous commit . Only use for: local changes . Amend Commits are not for commits that have been made public . Rollbacks . Revert to an earlier commit . With git revert, a new commit is created with inverse changes. This cancels previous changes instead of making it as though the original commit never happened. . &gt;&gt;git revert HEAD . Add description of why doing rollback . Recall press esc to exit insert mode, then type :x! (now the cursor is at the bottom) and hit enter to save and exit . Identifying a commit . SHA1 hash numbers that Git uses to identify commits . They provide the consistency that is critical for distributed systems such as Git. . | They are created using the commit message, date, author, and the snapshot taken of the working tree. . | They are composed of 40 characters. . | . &gt;&gt;git show # pick one of the commits &gt;&gt;git show 34051fe65ab25ae011b0473fd1707e6e84f89b71 # And if we wanted to revert to this &gt;&gt;git revert 34051fe65ab25ae011b0473fd1707e6e84f89b71 # this would also work if id is unique in repo &gt;&gt; git revert 34051f . Git Revert Cheat Sheet . git checkout is effectively used to switch branches. https://git-scm.com/docs/git-checkout | . git reset basically resets the repo, throwing away some changes. It‚Äôs somewhat difficult to understand, so reading the examples in the documentation may be a bit more useful. https://git-scm.com/docs/git-reset#_examples | . There are some other useful articles online, which discuss more aggressive approaches to resetting the repo. https://jwiegley.github.io/git-from-the-bottom-up/3-Reset/4-doing-a-hard-reset.html | . git commit --amend is used to make changes to commits after-the-fact, which can be useful for making notes about a given commit. https://git-scm.com/docs/git-commit#Documentation/git-commit.txtamend | . git revert makes a new commit which effectively rolls back a previous commit. It‚Äôs a bit like an undo command. https://git-scm.com/docs/git-revert | . There are a few ways you can rollback commits in Git. https://git-scm.com/book/en/v2/Git-Basics-Undoing-Things | . There are some interesting considerations about how git object data is stored, such as the usage of sha-1. | . Feel free to read more here: . A) https://en.wikipedia.org/wiki/SHA-1 . B) https://github.blog/2017-03-20-sha-1-collision-detection-on-github-com/ . Git Branch . a pointer to a particular commit | . it represents an independent line of development in a project | . the default branch that git creates for you when initialised is called master (sometimes main) | . To do something new or experimenting try adding a new branch | . New branches enable changes to be worked on without disrupting the most current working state | . List all branches in a repo . &gt;&gt;git branch * master . just the one at the moment the master . Add a new branch . &gt;&gt;git branch tryThis &gt;&gt;git branch * master tryThis . the * indicates the branch we are in . Switch to different branch . we use git checkout to check out the latest snapshot for both files and for branches. . &gt;&gt;git checkout tryThis Switched to branch &#39;tryThis&#39; &gt;&gt;git branch master * tryThis . Add and switch branches . &gt;&gt;git checkout -b orThis Switched to a new branch &#39;orThis&#39; &gt;&gt;git branch master * orThis tryThis . &gt;&gt; git log -2 commit 9204f0f890143d81c8fc3fa5db839be5f65e63e9 (HEAD -&gt; orThis) Author: thomashsimm &lt;thomas @gmail.com&gt; Date: Sun Oct 17 18:25:06 2021 +0100 add an empty file commit 34051fe65ab25ae011b0473fd1707e6e84f89b71 (tryThis, master) Author: thomashsimm &lt;thomas @gmail.com&gt; Date: Sun Oct 17 15:36:19 2021 +0100 Revert &quot;Revert &quot;added a gitignore file&quot;&quot; This reverts commit dc28347e0177c298e9b527ab8140973fb3a567ba. . We see the last two commits in this branch. Notice how next to the latest commit ID, git shows that this is where head is pointing to and that the branch is called orThis. . Next to the previous commit,git shows that both the master and the trThis branches are pointing to that snapshot of the project. . In this way, we can see that the orThis branch is ahead of the master branch. . &gt;&gt;nano gogo2.py #creates file gogo2.py &gt;&gt;git add * &gt;&gt;git commit -m &quot;gogo2.py file added to orThis&quot; &gt;&gt;ls -l total 2 -rw-r--r-- 1 44781 197609 9 Oct 17 18:56 gogo2.py -rw-r--r-- 1 44781 197609 38 Oct 17 14:51 test.py #go back to master &gt;&gt;git checkout master total 3 -rw-r--r-- 1 44781 197609 7 Oct 17 18:59 go.py -rw-r--r-- 1 44781 197609 16 Oct 17 18:59 gogo.py -rw-r--r-- 1 44781 197609 38 Oct 17 14:51 test.py #gogo2.py is not here! . Delete branch . &gt;&gt;git branch -d tryThis error: The branch &#39;tryThis&#39; is not fully merged. If you are sure you want to delete it, run &#39;git branch -D tryThis&#39;. &gt;&gt;git branch -D tryThis Deleted branch tryThis (was 59ec266). &gt;&gt;git branch * master orThis . Merging branches . Merging is the term that Git uses for combining branch data and history together. . &gt;&gt;git branch * master orThis #then merge orThis &gt;&gt;git merge orThis Updating da04c1c..4eb59cb Fast-forward go2.py | 4 ++++ 1 file changed, 4 insertions(+) create mode 100644 go2.py . &gt;&gt;git log commit 4eb59cb2dca6a09fb59477fcdf4595551ff9f85f (HEAD -&gt; master, branch2) Author: thomashsimm &lt;thomas @gmail.com&gt; Date: Sun Oct 17 19:17:28 2021 +0100 . Both branches are pointed at the same commit -&gt; (HEAD -&gt; master, branch2) . Merge conflict . From time to time, we might find that both the branches we&#39;re trying to merge have edits to the same part of the same file. . This will result in something called a merge conflict. . Normally, Git can automatically merge files for us. But when we have a merge conflict, it will need a little help to figure out what to do. . &gt;&gt;git merge branch2 Auto-merging go.py CONFLICT (content): Merge conflict in go.py Automatic merge failed; fix conflicts and then commit the result. #error message produced . &gt;&gt;git status On branch master You have unmerged paths. (fix conflicts and run &quot;git commit&quot;) (use &quot;git merge --abort&quot; to abort the merge) Unmerged paths: (use &quot;git add &lt;file&gt;...&quot; to mark resolution) both modified: go.py Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gogo.py.swp no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) . To try to resolve this we open up the file in editor . &gt;&gt;nano go.py ####in editor&gt;&gt; b=3 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD #hello ======= #comment here too &gt;&gt;&gt;&gt;&gt;&gt;&gt; branch2 ####which we change to this to keep both mods&gt;&gt; b=3 #hello #comment here too . Then add the file and check the status . &gt;&gt;git add go.py &gt;&gt;git status On branch master All conflicts fixed but you are still merging. (use &quot;git commit&quot; to conclude merge) Changes to be committed: modified: go.py &gt;&gt;git commit [master b7e6929] Merge branch &#39;branch2&#39; Kept both features . File &#34;&lt;ipython-input-2-5de9e5fcd226&gt;&#34;, line 1 &gt;&gt;git add go.py ^ SyntaxError: invalid syntax . Conflicts now gone . And committed . --graph --oneline ==&gt;&gt; helps us visualise the merge . &gt;&gt;git log --graph --oneline . . Abort merge . &gt;&gt;git merge --abort . Git Branches and Merging Cheat Sheet . git branch Used to manage branches https://git-scm.com/docs/git-branch . git branch [branchname] Creates the branch https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging . git branch -d [branchname] Deletes the branch https://git-scm.com/docs/git-branch#Documentation/git-branch.txt--D . git branch -D [branchname] Forcibly deletes the branch https://git-scm.com/docs/git-branch#Documentation/git-branch.txt--D . git checkout [branchname] Switches to a branch. https://git-scm.com/docs/git-checkout . git checkout -b [branchname] Creates a new branch and switches to it. https://git-scm.com/docs/git-checkout#Documentation/git-checkout.txt--bltnewbranchgt . git merge [branchname] Merge joins branches together. https://git-scm.com/docs/git-merge . git merge --abort If there are merge conflicts (meaning files are incompatible), --abort can be used to abort the merge action. . git log --graph --onelineThis shows a summarized view of the commit history for a repo. https://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-History . Working with remotes . GitHub is a web-based Git repository hosting service. . On top of the version control functionality of Git, GitHub includes extra features like bug tracking, wikis, and task management. . GitHub lets us share and access repositories on the web and copy or clone them to our local computer, so we can work on them. . GitHub is a popular choice with a robust feature set, but it&#39;s not the only one. Other services that provide similar functionality are BitBucket, and GitLab. . . GitHub provides free access to a Git server for public and private repositories. . It limits the number of contributors for the free private repositories, and offers an unlimited private repository service for a monthly fee. We&#39;ll be using a free repository for our examples, which is fine for educational use, small personal projects, or open source development. . Clone a directory . This is to create a local copy of a repository from the github website . 1) Go to the repo . 2) Click code and copy the clone URL HTTPS . other options are availabe too | . . 3) In Git bash run . git clone https://github.com/ThomasHSimm/SwanseaProperty.git . Fill in any passwords if required . #Then add and commit the changes &gt;&gt;git commit -a -m &quot;Modified the readme&quot; [main c9307f7] modified read me 1 file changed, 1 insertion(+) . Git Push . Send the changes to the remote repository . &gt;&gt;git push Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 8 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 309 bytes | 309.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To https://github.com/ThomasHSimm/SwanseaProperty.git 08aa0a2..c9307f7 main -&gt; main . Basic Interaction with GitHub Cheat-Sheet . There are various remote repository hosting sites: . - GitHub - BitBucket - Gitlab. . Follow the workflow at https://github.com/join to set up a free account, username, and password. After that, these steps (https://help.github.com/articles/create-a-repo/) will help you create a brand new repository on GitHub. . Some useful commands for getting started: . git clone URL . Git clone is used to clone a remote repository into a local workspace https://git-scm.com/docs/git-clone . git push . Git push is used to push commits from your local repo to a remote repo https://git-scm.com/docs/git-push . git pull . Git pull is used to fetch the newest updates from a remote repository https://git-scm.com/docs/git-pull . These can be useful for keeping your local workspace up to date. . https://help.github.com/en/articles/caching-your-github-password-in-git | . https://help.github.com/en/articles/generating-an-ssh-key | . Using a remote repository . to find the configuration of a remote repository go to the local copy and use git remote -v to find where the data is fetched from and pushed to. Below they are the same but can be different if say fetch with HTTP for read only and say SSH for pushing. But need to be same place. . &gt;&gt;git remote -v origin https://github.com/ThomasHSimm/THS_website.git (fetch) origin https://github.com/ThomasHSimm/THS_website.git (push) . And more information with git remote show origin . &gt;&gt;git remote show origin * remote origin Fetch URL: https://github.com/ThomasHSimm/THS_website.git Push URL: https://github.com/ThomasHSimm/THS_website.git HEAD branch: master Remote branches: gh-pages tracked master tracked Local branch configured for &#39;git pull&#39;: master merges with remote master Local ref configured for &#39;git push&#39;: master pushes to master (up to date) . If above had said master pushes to master (local out of date) . This means there have been changes on the remote and the local is now out of date- e.g. someone else has changed the repo. . We&#39;d now need to run &gt;&gt;git fetch to sync the data. . Find branches git is tracking git branch -r . To make changes to a remote branch we have to pull the remote branch, merge it with the local branch then push to the origin- the same thing we&#39;d do with the master. . Like with local change &gt;&gt;git status can give us changes in the remote repository if it exists (when in the local one) as well as the local changes . Fetching new changes . &gt;&gt;git fetch . This copies changes done in remote repo to local repo . But unlike git pull doesn&#39;t instantly merge changes . git pull = fetch and merge | git fetch = just fetches | . Can merge like this . &gt;&gt;git merge orgin/master . Or just do both with pull . As long as there are no conflicts, Git will move the current branch tip up to the target branch tip and combine histories of both commits. Using a fast-forward merge . &gt;&gt;git pull . To create a local branch for a remote branch we don&#39;t have yet run git checkout BRANCH_NAME . git remote update will fetch the contents of all remote branches and allow us to merge the contents ourselves. . Git Remotes Cheat-Sheet . git remote . Lists remote repos . git remote -v . List remote repos verbosely . git remote show [name] . Describes a single remote repo . git remote update . Fetches the most up-to-date objects . git fetch . Downloads specific objects . git branch -r . Lists remote branches; can be combined with other branch arguments to manage remote branches . Conflict Resolution Cheat Sheet . Merge conflicts are not uncommon when working in a team of developers, or on Open Source Software. Fortunately, GitHub has some good documentation on how to handle them when they happen: . https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-merge-conflicts . https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/resolving-a-merge-conflict-using-the-command-line . You can also use git rebase branchname to change the base of the current branch to be branchname . The git rebase command is a lot more powerful. Check out this link (https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History) for more information. . Git Fork and Pull Request Cheat Sheet . Check out the following link for more information: . https://help.github.com/en/articles/about-pull-request-merges . More Information on Code Reviews . Check out the following links for more information: . http://google.github.io/styleguide/ . https://help.github.com/en/articles/about-pull-request-reviews . https://medium.com/osedea/the-perfect-code-review-process-845e6ba5c31 . https://smartbear.com/learn/code-review/what-is-code-review/ . Additional Tools . Check out the following links for more information: . https://arp242.net/diy.html | . https://help.github.com/en/articles/closing-issues-using-keywords | . https://help.github.com/en/articles/setting-guidelines-for-repository-contributors | . https://www.infoworld.com/article/3271126/what-is-cicd-continuous-integration-and-continuous-delivery-explained.html | . https://stackify.com/what-is-cicd-whats-important-and-how-to-get-it-right/ | . https://docs.travis-ci.com/user/tutorial/ | . https://docs.travis-ci.com/user/build-stages/ | . . &lt;/div&gt; .",
            "url": "https://thomashsimm.com/2021/10/18/Git-and-Github.html",
            "relUrl": "/2021/10/18/Git-and-Github.html",
            "date": " ‚Ä¢ Oct 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Building A Classifier App With Weblink",
            "content": ". Building a Classifier App with Weblink . Thomas H. Simm, PhD . Creating an image classifier app that is on the net! Using Jupyter notebooks, FastAi, Binder and Voila categories: [fastai, jupyter, Binder, Voila, myBinder, ThomasHSimm] . 1) The Model Part . 2) The Python Part Of the App . 3) The Binder Part . 1) The model Part . The model was run on https://colab.research.google.com with a GPU Necessary to have a GPU for time . Some imports and installs . !pip install -Uqq fastbook import fastbook fastbook.setup_book() . Gets Azure search key to use Bing search API . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;keygoeshere&#39;) . I‚Äôm going to do a classifier for holiday types . destas={&#39;beach&#39;:{&#39;beach&#39;,&#39;tropical&#39;,&#39;sea&#39;,&#39;beach holidays&#39;}, &#39;snow&#39;:{&#39;ski&#39;,&#39;snowboard&#39;,&#39;snow&#39;,&#39;ski holidays&#39;}, &#39;countryside&#39;:{&#39;lakes mountains&#39;,&#39;countryside&#39;,&#39;forest&#39;,&#39;fields&#39;}, &#39;city&#39;:{&#39;city&#39;,&#39;cities&#39;,&#39;bars&#39;,&#39;buildings&#39;}, &#39;safari&#39;:{&#39;safari&#39;,&#39;safari holidays&#39;,&#39;safari park&#39;,&#39;safari africa&#39;}} . Creates a folder containing images for each type . Need to add to dir with different searches not delete and add new stuff each time . For eaach holiday type- go through the search topics and add the results together before downloading images to the holiday type folder . This gives 600 pics per holiday type . path = Path(&#39;Destinations&#39;) try: path.mkdir() except: pass . # resultsALL for o in destas: dest = (path/o) dest.mkdir(exist_ok=True) for ii,oo in enumerate(destas[o]): # print(oo) results = search_images_bing(key, f&#39;{oo}&#39;) if ii&gt;0: resultsALL = L(resultsALL,results).concat() else: resultsALL=results print(o) print(np.shape(resultsALL)) download_images(dest, urls=resultsALL.attrgot(&#39;contentUrl&#39;)) . Create a data block for fastai . fns = get_image_files(path) dests = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . get rid of failed images . failed = verify_images(fns) failed.map(Path.unlink) failed . Have a look at the images . dls = dests.dataloaders(path) dls.valid.show_batch(max_n=40, nrows=10) . . create a dls for the learner . randomresizedcrop | default aug transforms | . dests = dests.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = dests.dataloaders(path) . Do the learning over 4 epochs . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . . Confusion matrix . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . Save the model (saves as export.pkl) . learn.export() . 2) The App in python . Some imports . # Classifier App # THSimm from fastai.vision.all import * from fastai.vision.widgets import * import urllib.request . Create parts of the widget . learn_inf = load_learner(&#39;export.pkl&#39;) btn_upload = widgets.FileUpload() out_pl = widgets.Output() out_pl.clear_output() lbl_pred = widgets.Label() . Function occurs on click upload . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(500,500)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_click_classify,names=[&#39;data&#39;]) . What is displayed . #hide_output text =&#39;Select your plane&#39; display(VBox([widgets.HTML(value = f&quot;&lt;h1&gt;&lt;font color=&#39;Black&#39;&gt;{text}&lt;/h1&gt; &lt;ol text-align: center&gt;&lt;font color=&#39;Black&#39;&gt; &lt;li&gt;Beach&lt;/li&gt; &lt;li&gt;Snow&lt;/li&gt; &lt;li&gt;Countryside&lt;/li&gt; &lt;li&gt;City&lt;/li&gt; &lt;li&gt;Safari&lt;/li&gt; &lt;/ol&gt;&quot;), btn_upload, out_pl, lbl_pred]) ) . 3) The Binder Part . Binder . mybinder.org is an online service for building and sharing reproducible and interactive computational environments from online repositories. Under the hood, it is a federation of BinderHub deployments that are maintained by the Binder community. It serves as both a public service and a demonstration of the BinderHub technology, though it is by no means the only BinderHub in existence. If you‚Äôre interested in deploying your own BinderHub for your own uses, please see the BinderHub documentation and don‚Äôt hesitate to reach out to the Binder community. https://mybinder.readthedocs.io/en/latest/introduction.html#preparing-a-repository-for-binder . Basically allowing us to put code online . Voila . The 2nd import part is Voila which allows us to hide the code and just display outputs . https://voila.readthedocs.io/en/stable/using.html . Method: . Create repository on github that is public, containing the ipynb file and a requirements.txt file | Then go to binder https://mybinder.org/ | Fill in form as shown below | . In requirements.txt: . voila fastai packaging ipywidgets . . The result . https://mybinder.org/v2/gh/ThomasHSimm/LocationsClassifier/HEAD?urlpath=%2Fvoila%2Frender%2FAppForDestClass.ipynb . https://tinyurl.com/LocClassAppThomasHSimm . . .",
            "url": "https://thomashsimm.com/2021/10/11/Building-a-Classifier-App-with-weblink.html",
            "relUrl": "/2021/10/11/Building-a-Classifier-App-with-weblink.html",
            "date": " ‚Ä¢ Oct 11, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Building a Classifier App on the web",
            "content": ". 1) The Model Part . 2) The Python Part Of the App . 3) The Binder Part . 1) The model Part . The model was run on https://colab.research.google.com with a GPU . Necessary to have a GPU for time . Some imports and installs . !pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastbook import * from fastai.vision.widgets import * . Gets Azure search key to use Bing search API . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;keygoeshere&#39;) . I&#39;m going to do a classifier for holiday types . destas={&#39;beach&#39;:{&#39;beach&#39;,&#39;tropical&#39;,&#39;sea&#39;,&#39;beach holidays&#39;}, &#39;snow&#39;:{&#39;ski&#39;,&#39;snowboard&#39;,&#39;snow&#39;,&#39;ski holidays&#39;}, &#39;countryside&#39;:{&#39;lakes mountains&#39;,&#39;countryside&#39;,&#39;forest&#39;,&#39;fields&#39;}, &#39;city&#39;:{&#39;city&#39;,&#39;cities&#39;,&#39;bars&#39;,&#39;buildings&#39;}, &#39;safari&#39;:{&#39;safari&#39;,&#39;safari holidays&#39;,&#39;safari park&#39;,&#39;safari africa&#39;}} . Creates a folder containing images for each type . Need to add to dir with different searches not delete and add new stuff each time . For eaach holiday type- go through the search topics and add the results together before downloading images to the holiday type folder . This gives 600 pics per holiday type . path = Path(&#39;Destinations&#39;) try: path.mkdir() except: pass . for o in destas: dest = (path/o) dest.mkdir(exist_ok=True) for ii,oo in enumerate(destas[o]): # print(oo) results = search_images_bing(key, f&#39;{oo}&#39;) if ii&gt;0: resultsALL = L(resultsALL,results).concat() else: resultsALL=results print(o) print(np.shape(resultsALL)) download_images(dest, urls=resultsALL.attrgot(&#39;contentUrl&#39;)) . Create a data block for fastai . fns = get_image_files(path) dests = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . get rid of failed images . failed = verify_images(fns) failed.map(Path.unlink) failed . Have a look at the images . dls = dests.dataloaders(path) dls.valid.show_batch(max_n=40, nrows=10) . . create a dls for the learner . randomresizedcrop | default aug transforms | . dests = dests.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = dests.dataloaders(path) . Do the learning over 4 epochs . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . . Confusion matrix . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . Save the model (saves as export.pkl) . learn.export() . 2) The App in python . Some imports . # THSimm from fastai.vision.all import * from fastai.vision.widgets import * import urllib.request . Create parts of the widget . learn_inf = load_learner(&#39;export.pkl&#39;) btn_upload = widgets.FileUpload() out_pl = widgets.Output() out_pl.clear_output() lbl_pred = widgets.Label() . Function occurs on click upload . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(500,500)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; . btn_upload.observe(on_click_classify,names=[&#39;data&#39;]) . What is displayed . text =&#39;Select your plane&#39; display(VBox([widgets.HTML(value = f&quot;&lt;h1&gt;&lt;font color=&#39;Black&#39;&gt;{text}&lt;/h1&gt; &lt;ol text-align: center&gt;&lt;font color=&#39;Black&#39;&gt; &lt;li&gt;Beach&lt;/li&gt; &lt;li&gt;Snow&lt;/li&gt; &lt;li&gt;Countryside&lt;/li&gt; &lt;li&gt;City&lt;/li&gt; &lt;li&gt;Safari&lt;/li&gt; &lt;/ol&gt;&quot;), btn_upload, out_pl, lbl_pred]) ) . 3) The Binder Part . Binder . mybinder.org is an online service for building and sharing reproducible and interactive computational environments from online repositories. Under the hood, it is a federation of BinderHub deployments that are maintained by the Binder community. It serves as both a public service and a demonstration of the BinderHub technology, though it is by no means the only BinderHub in existence. If you‚Äôre interested in deploying your own BinderHub for your own uses, please see the BinderHub documentation and don‚Äôt hesitate to reach out to the Binder community.https://mybinder.readthedocs.io/en/latest/introduction.html#preparing-a-repository-for-binder . Basically allowing us to put code online . Voila . The 2nd import part is Voila which allows us to hide the code and just display outputs . https://voila.readthedocs.io/en/stable/using.html . Method: . Create repository on github that is public, containing the ipynb file and a requirements.txt file | Then go to binder https://mybinder.org/ | Fill in form as shown below | . In requirements.txt: . voila fastai packaging ipywidgets . . The result . https://mybinder.org/v2/gh/ThomasHSimm/LocationsClassifier/HEAD?urlpath=%2Fvoila%2Frender%2FAppForDestClass.ipynb . https://tinyurl.com/LocClassAppThomasHSimm . . .",
            "url": "https://thomashsimm.com/fastai/jupyter/binder/voila/mybinder/thomashsimm/2021/10/10/ImageClassifier.html",
            "relUrl": "/fastai/jupyter/binder/voila/mybinder/thomashsimm/2021/10/10/ImageClassifier.html",
            "date": " ‚Ä¢ Oct 10, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Python Learnings",
            "content": ". . Introducing Python Object Types . Based on Chapter 4 of &quot;Learning Python&quot;, 4th Edition, Mark Lutz, O&#39;Reilly . Some maths . import math print(math.pi) print(math.sqrt(34)) . import random print(random.random()) print(random.choice([1,2,3,4])) print(random.choice([1,2,3,4])) . Indexing . S=&#39;Spam&#39; print(S[1:3]) print(S[:-1]) print(S[1:]) print(S[:]) print(S*3) . Immutability . # strings are not immutable S[0]=&#39;p&#39; ##error . S = &#39;z&#39; +S[1:] print(S) #or use replaces . String specific methods . print(S.find(&#39;pa&#39;))#finds position #replace parts of a string print(S) print(S.replace(&#39;pa&#39;,&#39;XYZ&#39;)) #upper and lower case print(S.upper()) print(S.lower()) . line=&#39;aaa,bbb,cccc,dd d n&#39; #split based on something print(line.split(&#39;,&#39;)) #creates a list print(type(line.split(&#39;,&#39;))) # strip out whitespace on rhs print(line.rstrip()) . Formatting . print(&#39;%s, eggs, and %s&#39; % (&#39;spam&#39;, &#39;SPAM!&#39;)) print(&#39;{0}, eggs, and {1}&#39;.format(&#39;spam&#39;, &#39;SPAM!&#39;)) . Help . Put into help( ) to get help on it . help(S.format) . Lists . The Python list object is the most general sequence provided by the language. Lists are positionally ordered collections of arbitrarily typed objects, and they have no fixed size. They are also mutable‚Äîunlike strings, lists can be modified in-place by assignment to offsets as well as a variety of list method calls . #list of different types L =[123, &#39;spam&#39;,1.23] print(L) #access print(L[2]) #append L.append(&#39;NI&#39;) print(L) #get rid of one pop! L.pop(0) print(L) M=[&#39;aa&#39;,&#39;jeji&#39;,&#39;boio&#39;,&#39;popo&#39;,&#39;gsss&#39;,&#39;zulu&#39;,&#39;ccc&#39;] #sort M.sort() print(M) M.reverse() print(M) . Comprehensions . Python includes a more advanced operation known as a list comprehension expression, which turns out to be a powerful way to process structures like our matrix. . M=[[1, 2, 3], [4, 5,6], [7, 8, 9]] print(M[1][2]) # this is a comprehension col1 = [row[1] for row in M] print(col1) #put if statement in print([row[1] for row in M if row[1]%2 ==0]) . print(M[0][:]) # we can&#39;t do this! for a column M[1,:] . Dictionaries . Python dictionaries are something completely different (Monty Python reference intended)‚Äîthey are not sequences at all, but are instead known as mappings. Mappings are also collections of other objects, but they store objects by key instead of by relative position. In fact, mappings don‚Äôt maintain any reliable left-to-right order; they simply map keys to associated values. Dictionaries, the only mapping type in Python‚Äôs core objects set, are also mutable: they may be changed in-place and can grow and shrink on demand, like lists. . D = {&#39;food&#39;:&#39;Spam&#39;,&#39;quality&#39;:4, &#39;color&#39;:&#39;pink&#39;} print(D) #or create by key assignment D={} D[&#39;food&#39;]=&#39;Spam&#39; D[&#39;quality&#39;]=4 D[&#39;color&#39;]=&#39;pink&#39; print(D) #index it print(D[&#39;food&#39;]) . #what if the info is more complex? Nest rec = {&#39;name&#39;:{&#39;first&#39;:&#39;Bob&#39;,&#39;last&#39;:&#39;Smith&#39;}, &#39;job&#39;:[&#39;dev&#39;,&#39;mgr&#39;], &#39;age&#39;:40.5} #index them print(rec[&#39;name&#39;]) print(rec[&#39;name&#39;][&#39;last&#39;]) print(rec[&#39;job&#39;][0]) #or add more NB job is a list rec[&#39;job&#39;].append(&#39;janitor&#39;) print(rec) #keys are 1st bit print(rec.keys()) . Tuples . roughly like a list that cannot be changed‚Äîtuples are sequences, like lists, but they are immutable, like strings. Syntactically, they are coded in parentheses instead of square brackets, and they support arbitrary types, arbitrary nesting, and the usual sequence operations: . T=(1,2,3,4) print(len(T)) #concatenation print(T+(5,6)) #indexing print(T[0]) ## or T = (&#39;spam&#39;, 3.0, [11, 22, 33]) print(T) . Numeric Types . Based on Chapter 5 of &quot;Learning Python&quot;, 4th Edition, Mark Lutz, O&#39;Reilly . print(type(3)) print(type(3.0)) . print(102.2//3.2) #normal division print(102.2/3.2) #to the power print(3**2) #remainder print(100%3) #complex numbers j or J print((1j +2)*3J) . (52-6)*5*7.5*10*.89 10*.89*5*7.5 . Class Methods . Classes and Instances . Classes define the behavior of all instances of a specific class. . | Each variable of a specific class is an instance or object. . | Objects can have attributes, which store information about the object. . | You can make objects do work by calling their methods. . | The first parameter of the methods (self) represents the current instance. . | Methods are just like functions, but they can only be used through a class. . | . Special methods . Special methods start and end with __. (two underscores) . | Special methods have specific names, like init for the constructor or str for the conversion to string. . | . Defining a class . class ClassName: def method_name(self, other_parameters): body_of_method . Defining a class with a method . class ClassName: &quot;&quot;&quot;Documentation for the class.&quot;&quot;&quot; def method_name(self, other_parameters): &quot;&quot;&quot;Documentation for the method.&quot;&quot;&quot; body_of_method . Object Composition . You can have a situation where two different classes are related, but there is no inheritance going on. This is referred to as composition -- where one class makes use of code contained in another class. For example, imagine we have a Package class which represents a software package. It contains attributes about the software package, like name, version, and size. We also have a Repository class which represents all the packages available for installation. While there‚Äôs no inheritance relationship between the two classes, they are related. The Repository class will contain a dictionary or list of Packages that are contained in the repository. Let&#39;s take a look at an example Repository class definition: . class Repository: ... def __init__(self): ... self.packages = {} ... def add_package(self, package): ... self.packages[package.name] = package ... def total_size(self): ... result = 0 ... for package in self.packages.values(): ... result += package.size ... return result . In the constructor method, we initialize the packages dictionary, which will contain the package objects available in this repository instance. We initialize the dictionary in the constructor to ensure that every instance of the Repository class has its own dictionary. . We then define the add_package method, which takes a Package object as a parameter, and then adds it to our dictionary, using the package name attribute as the key. . Finally, we define a total_size method which computes the total size of all packages contained in our repository. This method iterates through the values in our repository dictionary and adds together the size attributes from each package object contained in the dictionary, returning the total at the end. In this example, we‚Äôre making use of Package attributes within our Repository class. We‚Äôre also calling the values() method on our packages dictionary instance. Composition allows us to use objects as attributes, as well as access all their attributes and methods. .",
            "url": "https://thomashsimm.com/2021/10/04/PythonBook.html",
            "relUrl": "/2021/10/04/PythonBook.html",
            "date": " ‚Ä¢ Oct 4, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "The operating system",
            "content": ". import os import sys . name of operating system . print(&#39;os.name&#39;,&#39;,&#39;,os.name,&#39; n&#39;) . os.name , nt . get current dir . print(&#39;os.getcwd()&#39;,&#39;,&#39;,os.getcwd(),&#39; n&#39;) . os.getcwd() , C: Users 44781 pyproj _misc . list files in dir . print(&#39;os.listdir()&#39;,&#39;,&#39;,os.listdir(),&#39; n&#39;) . os.listdir() , [&#39;.ipynb_checkpoints&#39;, &#39;adapic.jpg&#39;, &#39;awarhol.jpg&#39;, &#39;awarhol.webp&#39;, &#39;dog.jpg&#39;, &#39;draw.png&#39;, &#39;ebsd2.tif&#39;, &#39;ebsdmap.png&#39;, &#39;ebsdmap.tif&#39;, &#39;fastAI_C1_notes.ipynb&#39;, &#39;image_1000.jpg&#39;, &#39;image_5000(1).jpg&#39;, &#39;IndeedExtract-Copy1.ipynb&#39;, &#39;IndeedExtract-Copy2.ipynb&#39;, &#39;No_61_Mark_Rothko-thumbnail_webp-9999x9999.webp&#39;, &#39;os.ipynb&#39;, &#39;output&#39;, &#39;PF_500C R.png&#39;, &#39;PythonBook.ipynb&#39;, &#39;re.ipynb&#39;, &#39;styleTransfer.ipynb&#39;, &#39;test2&#39;, &#39;Untitled.ipynb&#39;, &#39;vangogh.jpg&#39;, &#39;vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;, &#39;water.webp&#39;] . make a directory . this_dir=os.getcwd() directory=&#39;test&#39; #this adds either / or depending on os path = os.path.join(this_dir, directory) try: os.mkdir(path) except: pass #or try: os.mkdir(&#39;test2&#39;) except: pass . list files in dir . print(&#39;os.listdir()&#39;,&#39;,&#39;,os.listdir(),&#39; n&#39;) . os.listdir() , [&#39;.ipynb_checkpoints&#39;, &#39;adapic.jpg&#39;, &#39;awarhol.jpg&#39;, &#39;awarhol.webp&#39;, &#39;dog.jpg&#39;, &#39;draw.png&#39;, &#39;ebsd2.tif&#39;, &#39;ebsdmap.png&#39;, &#39;ebsdmap.tif&#39;, &#39;fastAI_C1_notes.ipynb&#39;, &#39;image_1000.jpg&#39;, &#39;image_5000(1).jpg&#39;, &#39;IndeedExtract-Copy1.ipynb&#39;, &#39;IndeedExtract-Copy2.ipynb&#39;, &#39;No_61_Mark_Rothko-thumbnail_webp-9999x9999.webp&#39;, &#39;os.ipynb&#39;, &#39;output&#39;, &#39;PF_500C R.png&#39;, &#39;PythonBook.ipynb&#39;, &#39;re.ipynb&#39;, &#39;styleTransfer.ipynb&#39;, &#39;test&#39;, &#39;test2&#39;, &#39;Untitled.ipynb&#39;, &#39;vangogh.jpg&#39;, &#39;vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;, &#39;water.webp&#39;] . Check file/dir exists . print(os.path.exists(&quot;test2&quot;),os.path.exists(&quot;test&quot;)) . True True . change directory . os.chdir(directory) print(&#39;os.getcwd()&#39;,&#39;,&#39;,os.getcwd(),&#39; n&#39;) . os.getcwd() , C: Users 44781 pyproj _misc test . Create and Rename a file . open(&#39;Old.txt&#39;,&#39;a&#39;).close() # check file exists print(&#39;old=&#39;,os.path.exists(&quot;Old.txt&quot;),&#39;. new=&#39;,os.path.exists(&quot;New.txt&quot;)) # rename a file fd = &quot;Old.txt&quot; os.rename(fd,&#39;New.txt&#39;) # check file exists print(&#39;old=&#39;,os.path.exists(&quot;Old.txt&quot;),&#39;. new=&#39;,os.path.exists(&quot;New.txt&quot;)) . old= True . new= False old= False . new= True . Copy a file- . can be done in os but easier in shutil . https://stackabuse.com/how-to-copy-a-file-in-python/ . import shutil shutil.copyfile(&#39;New.txt&#39;, &#39;Old.txt&#39;) # check file exists print(&#39;old=&#39;,os.path.exists(&quot;Old.txt&quot;),&#39;. new=&#39;,os.path.exists(&quot;New.txt&quot;)) . old= True . new= True . Remove a file . os.remove(&quot;New.txt&quot;) # os.remove(&quot;Old.txt&quot;) # check file exists print(&#39;old=&#39;,os.path.exists(&quot;Old.txt&quot;),&#39;. new=&#39;,os.path.exists(&quot;New.txt&quot;)) . old= True . new= False . Go back up in directory . os.path.dirname(os.path.dirname( )) . &#39;C: Users 44781 pyproj&#39; . os.rmdir(path) #list files in dir print(&#39;os.listdir()&#39;,&#39;,&#39;,os.listdir(),&#39; n&#39;) . os.listdir() , [&#39;.ipynb_checkpoints&#39;, &#39;adapic.jpg&#39;, &#39;awarhol.jpg&#39;, &#39;awarhol.webp&#39;, &#39;dog.jpg&#39;, &#39;draw.png&#39;, &#39;ebsd2.tif&#39;, &#39;ebsdmap.png&#39;, &#39;ebsdmap.tif&#39;, &#39;fastAI_C1_notes.ipynb&#39;, &#39;image_1000.jpg&#39;, &#39;image_5000(1).jpg&#39;, &#39;IndeedExtract-Copy1.ipynb&#39;, &#39;IndeedExtract-Copy2.ipynb&#39;, &#39;No_61_Mark_Rothko-thumbnail_webp-9999x9999.webp&#39;, &#39;os.ipynb&#39;, &#39;output&#39;, &#39;PF_500C R.png&#39;, &#39;PythonBook.ipynb&#39;, &#39;re.ipynb&#39;, &#39;styleTransfer.ipynb&#39;, &#39;test2&#39;, &#39;Untitled.ipynb&#39;, &#39;vangogh.jpg&#39;, &#39;vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;, &#39;water.webp&#39;] . Create new files . nano file.txt #create file touch file.txt . !cd . C: Users 44781 pyproj _misc test . Subprocess module . import subprocess # subprocess.run(&quot;date&quot;,shell=True) print(subprocess.run([&#39;cmd&#39;, &#39;/c&#39;, &#39;date&#39;])) . CompletedProcess(args=[&#39;cmd&#39;, &#39;/c&#39;, &#39;date&#39;], returncode=1) . Pointers for Getting Your Environment Setup . Learning more about operating systems . We‚Äôve talked briefly about what an operating system is and what we&#39;ll need to know about operating systems for this course. If you want to learn some additional operating system concepts, check out the videos on this subject in the Technical Support Fundamentals course (https://www.coursera.org/lecture/technical-support-fundamentals/module-introduction-I3n9l). If you want to dive deeper onto how to manage Windows and Linux, check out the Operating Systems and You: Becoming a Power User course (https://www.coursera.org/learn/os-power-user). . If you want to discover more about the history of Unix, you can read all the details on the Unix Wikipedia page https://en.wikipedia.org/wiki/History_of_Unix. . Installing Python and additional modules . If you don&#39;t have Python installed yet, we recommend that you visit the official Python website (http://www.python.org/) and download the installer that corresponds to your operating system. . There‚Äôs a bunch of guides out there for installing Python and they all follow a similar process to the one we described in the videos. This guide from Real Python (https://realpython.com/installing-python/) includes instructions on how to install python on a range of different operating systems and distributions. . Once you have Python installed on your operating system, it&#39;s a good idea to familiarize yourself with pip and the associated tools. You can find more info about these here https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/. . Using package management systems . Package management systems help you better manage the software installed on your machine. These management systems vary a lot from operating system to operating system. So, you need to pick the one that works for the OS you‚Äôre using. Check out these guides for help with this: . Installing Python 3 on Windows 10 with Chocolatey https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-windows-10 | . Installing Python 3 on MacOS with Homebrew http://www.pyladies.com/blog/Get-Your-Mac-Ready-for-Python-Programming/ | . Package management basics on Linux https://www.digitalocean.com/community/tutorials/package-management-basics-apt-yum-dnf-pkg | . Other information . Python in the Microsoft Store for Windows 10 https://devblogs.microsoft.com/python/python-in-the-windows-10-may-2019-update/ | . Setting up Your Environment . After you‚Äôve installed Python and checked that it works, the next step to set up your developer environment is to choose your main code editor. . These are some of the common editors for Python, available for all platforms: . Atom https://atom.io/ . | Eclipse https://www.eclipse.org/ . | PyCharm https://www.jetbrains.com/pycharm/ . | Sublime Text http://www.sublimetext.com/ . | Visual Studio Code https://code.visualstudio.com/ . | . You can read more about these editors, and others, in these overview comparatives: . Python IDEs and Code Editors (Guide) https://realpython.com/python-ides-code-editors-guide/#pycharm . | Best Python IDEs and Code Editors https://www.softwaretestinghelp.com/python-ide-code-editors/ . | Top 5 Python IDEs for Data Science https://www.datacamp.com/community/tutorials/data-science-python-ide . | . We encourage you to try out these editors and pick your favorite. Then, install it on your computer and experiment with writing and executing Python scripts locally. . Reading and Writing Files Cheat-Sheet . Check out the following link for more information: . https://docs.python.org/3/library/functions.html#open | . Files and Directories Cheat-Sheet . Check out the following links for more information: . https://docs.python.org/3/library/os.html . | https://docs.python.org/3/library/os.path.html . | https://en.wikipedia.org/wiki/Unix_time . | . CSV Files Cheat Sheet . Check out the following links for more information: . https://docs.python.org/3/library/csv.html . | https://realpython.com/python-csv/ . | .",
            "url": "https://thomashsimm.com/2021/10/04/OS.html",
            "relUrl": "/2021/10/04/OS.html",
            "date": " ‚Ä¢ Oct 4, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Swansea House Price Report",
            "content": "An investigation of the influence of location and property characteristics in Swansea, UK . Overview . An investigation of the effect of the details (or characteristics) of a property (e.g., room size) and its nearness to different location venues (e.g. shops), on the property prices in Swansea, UK. . The Swansea area is characterised by details of its households, both property and location details. This information is used to show the importance these aspects have on property price and create house price predictions. Cluster analysis is performed to look at the characteristics of different areas. . The investigation is an exploratory analysis of the capabilities of machine learning and the foursquare location service (https://foursquare.com/). The report is produced for the final submission of the IBM Data Science Professional Certificate [https://www.coursera.org/professional-certificates/ibm-data-science]. Some of the data and python files involved in the analysis are given at https://github.com/dMaterialia/SwanseaProperty. Some other links involving interactive plots are given within the report. . Keywords: Swansea, Property, FourSquare, IBM Data Science, Machine Learning, Clustering, Random Forrest Model . Contents . 1 Introduction 3 . 2 Data and Methods 5 . 2.1 Data 5 . 2.1.1 Census Data 5 . 2.1.2 Doogal data 5 . 2.1.3 Four Square 6 . 2.1.4 Location Polygons 6 . 2.2 Methodology 6 . 2.2.1 Property Prices 6 . 2.2.2 Selected Areas 7 . 2.2.3 FourSquare 8 . 2.2.4 Swansea Data Plots 9 . 2.2.5 Machine Learning Algorithms 10 . 2.2.6 Post processing 11 . 3 Results 11 . 3.1 Property Details 11 . 3.2 Location details 15 . 3.2.1 Locations Using Search 15 . 3.2.2 Locations Using Explore 17 . 3.3 Location and Property details 18 . 4 Discussion 19 . 4.1 The Selected Areas 19 . 4.2 Effect of number of rooms 20 . 4.3 Quantifying the Location Effect 21 . 4.4 Is the model and data a good predictor of property prices? 22 . 4.5 Getting more out of clustering 23 . 5 Conclusion 23 . 6 References 24 . Introduction . Swansea is a city in Wales in the United Kingdom. It is the second largest city in Wales and the twenty-fifth largest city in the United Kingdom, with a population of 241,300 in 2014 [1]. The city is located by the sea, on the Bristol Channel. Within the Swansea region there is a mixture of location types including seaside, maritime, residential, shopping, industrial, and agricultural (Figure 1). . The city‚Äôs location relative to the sea and the nearest cities in Wales and England have an important influence on the city. The nearest city is Cardiff the capital of Wales, 68 km to the west, whereas to the east St. Davids is the nearest city at 110 km and to the north St Asaph at 220 km away is the nearest city. The population density in the surrounding regions is therefore relatively low compared to the rest of the UK (Figure 2) and Swansea is a relatively remote location to other places in the UK. . . Figure . Images of Swansea. Sources various. . Within the Swansea city region being investigated the nature of the individual regions varies considerably. To the west is the Gower peninsula a coastal region which is designated as an Area of Outstanding Natural Beauty [2]. Whereas, to the north and east are regions with a high density of industrial estates. There are also areas known for their nightlife including Uplands and Mumbles as well as the city centre region. The Swansea marina district is a thriving residential area near the city centre consisting of a high density of flats, which replaced the previous industrial area that had declined from 1970s onwards [3]. In addition, the student population has an important influence on the city, with a high-density student population to the west of the city in the Uplands and Brynmill areas. In between these areas are a variety of residential areas, with varying deprivation and property prices. . . Figure . Population density of the UK, arrow highlights Swansea. Image from [4]. . In the property world, the cliche is that the three most important factors in the desirability of a house are ‚Äúlocation, location, location‚Äù [5]. The location of Swansea relative to the rest of the UK and the location of the parts of Swansea to its local geography (e.g., the beach, city centre, marina) have shaped the nature of the areas within it. One of the questions that this work seeks to answer is how the current local geography dictates the property prices. How true is ‚Äúlocation, location, location‚Äù and can this effect be quantified relative to the details of a property? What are the different aspects about location that matter? And to what extent? What can be used to guide property investment or government planning? This can be done by considering what details may change in the future and what cannot (e.g., distance to a beach versus distance to local shop). . It should be noted that socio-economics or crime statistics are purposely ignored, this makes the problem simpler but is also done because this is not what is being investigated. However, the simplicity of the data used may be at a detriment in terms of predictions. These details could be relatively easily included, as deprivation is included in one of the datasets and the crime statistics are readily available. However, property price and deprivation or crime statistics are well linked. Hence, including these variables may mask other effects of location or property details. . The work will try to address: . How much of the property price can be ascertained from details of the property, such as number of rooms, alone? . | And can the influence of location on price be quantified based on the businesses, facilities and environment situated nearby? . | . Property prices will be used as a target value, as an indication of the property‚Äôs desirability. The other values will be used as parameters to understand this. . Data and Methods . Data . The two main sources of data in this report are the UK Census data and the foursquare.com database. In addition, the website doogal.co.uk and UK data service are also used. . Census Data . The UK census is a data collection exercise in the United Kingdom which occurs every 10 years. Participation in the census is compulsory and is used to obtain statistics on the UK‚Äôs economy and society, which is then used to assist the planning and allocation of resources, policy-making and decision-making [6]. Details of the census can be accessed at the website [7]. Mainly to maintain anonymity of participants, the census data is separated into areas which are unique to the census. The areas are larger than single postal code regions, e.g., SA2 0DE, but smaller than the first digits of the postcode e.g., SA2 or district regions such as Uplands in Swansea. . In the Swansea area there are ~9,000 unique postcodes. Whereas for the UK census areas there are ~1,000 regions in Swansea. Most analysis will be done on these census areas to allow data to be merged from different sources. . As shown in Figure 3 lots of information can be obtained from the census data. Based on the remit used here, only 3 of these datasets were used: 1) KS101EW- Usual resident population, 2) KS401EW- Dwellings, household spaces and accommodation type, 3) KS403EW- Rooms, bedrooms, and central heating. Within these files only some of the data is used. The categories extracted are shown in Table 1. . Table . Categories used from Census data about property details. . No. of People | Area (hect.) | Density (ppl/hect.) | Rooms Per House | Bedrooms . Per House . | % No central . heat . | No. of Dwell.s | % Detached | % Semi Detached | % Terraced | % Flats | % Mobile | . . Figure . Screen shot of the Census data page used [8]. . Doogal data . There is some cross-over between with the Census data as some of the same data sources are used. But Doogal will mainly be used for: . To match postcodes to census ID . | Location data- latitude and longitude . | Property price information . | . Four Square . For location information FourSquare [9] will be used. Instead of using the explore function, several key locations have been identified that can characterise an area. These include: . Beach . | Parks . | Pubs . | Supermarkets . | Doctors . | etc . | . Approximately 20 categories will be used, which will mean ~20, 000 calls to foursquare. The reason for this method is that the explore approach was not able to uniquely identify regions, for example large cluster sizes were needed. More detail is provided in the methods section. . Location Polygons . Location polygons were found from the UK data service [10] this allows plotting of Choropleth Maps. Choropleth Maps are location plots where different regions are colour coded based on a parameter such as house price. To do this polygons of the regions are required, normally in in .KML or .shp file formats. A slight modification of the Census polygons was done to convert .shp files to .json files for use in Folium which is shown in the file ‚ÄúcreateSwanseaJSON_fromGov.ipynb‚Äù. . Methodology . All analysis is done using Python 3.8. Both Spyder and JupyterLabs are used. Spyder to create .py function files which are then called by the notebooks in JupyterLabs for visualisation of the results. Most of the scripts and data can be accessed on my GitHub page for this project [11]. The interactive nature of folium plots are difficult to share, the best way around this is to use nbviewer [12] to share notebooks put on github. . Property Prices . The property data exists as sales data for each post code over several years. Since, property price is the key variable in this report some consideration of how to deal with this is needed. Given inflation of property prices, this data is not ideal. Data sources such as Zoopla which create an expected price of a property; however, this cannot be used here as they forbid webscraping. Instead, to use the property sales data property inflation will have to be accounted for. A simple way to do this, and that used here, is to take the average over a period after the financial crash and before a current upsurge in prices when prices were approximately level (Figure 4). Between 2008 and 2019 is used. . A secondary data filtering was attempted, whereby the house prices where adjusted based on the percentage of house types in the Census data. i.e., Avg. House Price = %Terraced x Avg. Terrace price + %Detached x Avg. Detached price + etc. However, given the sparsity in some areas for property sales it gave undue weight to some individual house sales. For this reason, the overall average was used. A more refined procedure accounting for the number of sales in each property type that uses general trends in prices in different property types across Swansea would be an improvement. In addition, a weighting of predictions based on confidence of property prices would be another improvement. . . Figure . Change in property price for different post code areas. All property sales are used, and a moving average is used. . As is shown in Figure 5. There is a considerable variation in property prices across Swansea. . . Figure . Price of properties in Swansea Census regions. . Selected Areas . To help break-down the data, five regions within Swansea have been selected to provide additional information. These are shown in Figure 6. . The Langland region is situated to the west of the city on the Gower peninsula close to a beach. . | The Penderry region ranks low in the Census deprivation index, situated to the north of the region. . | The Morriston North region is a middling region within Swansea, situated in the north of the region and close to the M5 motorway. . | Uplands is one of the most popular areas outside the centre for its nightlife and shops, it also has a high student density. . | The Swansea Marina area is a relatively new region within Swansea, emerging after the deindustrialisation of the area, consisting mainly of flats. The Maritime region chosen is one of the more expensive Census regions in this area. . | . . Figure . Map of Swansea area. The city centre and locations of post-code regions that will be looked at in more detail are shown. . FourSquare . Foursquare is used to get information about venues close to the census regions. Venues include a wide range of places including shops, offices, beaches, sports centres (a full list is found here [13]. The venues are separated with a venue hierarchy, so for example for a football stadium, is in the stadium category which is itself in the Arts &amp; Entertainment category. One way this hierarchy caused problems was in separating out restaurants from fast food venues, where each specific venue had to be selected. . Foursquare has two main venue searches: . Search: returns a list of venues near a given location [14] . | Explore: returns a list of recommended venues near a given location [15] . | . Both have some customization on the criteria searched, based on the type of venue as well as other criteria. This is particularly the case for ‚Äòexplore‚Äô where time of day/week, popularity of venue and price can be considered. Venue categories can be limited both by providing a list of categories and in ‚Äòexplore‚Äô only by specifying a section which includes: ‚Äúfood, drinks, coffee, shops, arts, outdoors, sights, trending, nextVenues (venues frequently visited after a given venue), or topPicks (a mix of recommendations generated without a query from the user)‚Äù [15]. . Two methods are used, one using ‚Äòsearch‚Äô to get selected venues near the census regions, and a second using ‚Äòexplore‚Äô using the different section categories. ‚ÄòSearch‚Äô is used for supervised machine learning to predict property price. And ‚Äòexplore‚Äô for unsupervised learning to categorise clusters based on location. . For the ‚Äòsearch‚Äô call 24 venue categories were chosen (shown in Table 2) and either the distance to this venue type (given as Distance in the table) or how many are within a given radius of the census region (given as Frequency in the table). The distance metric is the radius over which these are determined. Further details of this processing is in the file ‚ÄúClusteringByLocation.py‚Äù in the function ‚Äúfit_Clusters‚Äù. . Table . The different venue categories used in the ‚Äòsearch‚Äô call. ‚ÄòDistance‚Äô means that the distance to a venue is measured, whereas ‚ÄòFrequency‚Äô gives the number of those venues within the given radius. . . For the ‚Äòexplore‚Äô call each of the ‚ÄòSections‚Äô are used with a search radius of 800 m, the time and day were set as ‚Äòany‚Äô, and sortByPopularity as 1 (i.e. venues at any time sorted by popularity). The data requires additional processing to incorporate into a machine learning model. First one hot encoding is used to convert the categorical data about venue details (a list of venue types for each location) to a form as shown in Figure 7, whereby a 1 indicates the venue category exists. Then for each location the rows are added and normalised, so that the number in a row represents the fraction of that venue type found in that area. So, in the figure area W00010265 has 3.8% American restaurants, 1.9% BBQ joints. This is the data used in the model. To help with categorising the data is then converted to a list of the most common venue types, as shown in the figure. Further details of this processing is in the file ‚ÄúClusteringByLocation.py‚Äù in the function ‚ÄúClustering‚Äù. . . . Figure . Output from data processing after details of venues have been determined. Top-left, one hot encoding, top-right convert to a fractional values for modelling, bottom, covert to help categorising and plotting. . Swansea Data Plots . Plots of Swansea to visualise the results were performed with Folium [16]. Two types of plots were used Choropleth maps, where the region is coloured based on a scalar quantity, and Cluster plots, where the colour of marker shows what cluster an area is in. . For the latter two colour arrays (rainbow and rainbow2) were created for the inner and outer colour of the markers using seaborn, as follows: . # set color scheme for the clusters . N = kclusters+5#add a bit as color returns to same . import seaborn as sns . import copy . import random . colors_array = sns.color_palette(‚Äòhls‚Äô, N)#husl hls Paired . rainbow = [colors.rgb2hex(i) for i in colors_array] . rainbow2=copy.copy(rainbow) . random.shuffle(rainbow) . Machine Learning Algorithms . Both supervised and unsupervised models were used. . Figure 8 displays a handy guide for choosing a machine learning algorithm, taken from [17]. For the trained models, we want to predict a value. Therefore, as shown in the figure a linear regression model is a fast method to use. For comparison a Decision Forest Regression is used, which has a high accuracy whilst also having fast training times. The third method used for supervised learning, k-nearest neighbour regression, was chosen because the algorithm is like the one used for the unsupervised learning, and may provide different insights in the data. For the unsupervised learning model, a model was wanted that could cluster data and reveal structures. For this there is one clear choice from the figure, K-means clustering. . The supervised models used property price as a target. For all models, data was split into test and train datasets with 70% of the data used for training. All reported errors in results are from applying the model to the test dataset. A linear regression model was used on the property details, but due to its accuracy compared to other methods was not used elsewhere. A random forest regressor model (RandomForestRegressor) and a K nearest neighbours regressor (KNeighborsRegressor) are the main models used for supervised modelling. For the RandomForestRegressor the number of trees in the forest is determined by finding the minimum mean absolute error using a range of values for the number of trees (up to 600). For the KNeighborsRegressor the number of nearest neighbours was found using GridSearch for neighbours up to 15 (values normally found were ~4). . An unsupervised model was used on the location data to cluster different areas. For this a K-Means clustering (Kmeans) model was used. To determine the number of clusters to form, the ‚Äòelbow method‚Äô was used. This was done by calculating the square of Euclidean distance of each point from its cluster centre for different number of clusters. The two are plotted against each other and the value chosen is where there is a noticeable flattening in the curve (the elbow). . All models used are from scikit-learn [18]. . . Figure . Machine learning algorithm cheat sheet, taken from [17]. . Post processing . To help understand the results of the supervised learning models more and find out about the effect of location and number of rooms an additional processing is performed. The way this is done is by altering the data slightly and giving it back to the model. . For example, to understand the location effect on property prices, the average property details are given to the model alongside the region‚Äôs location details. This can be seen in the function ‚Äúlocationeffect‚Äù in ‚ÄúMachineLearnBig.py‚Äù. Or to quantify the effect of the number of rooms by changing the number of rooms whilst keeping other parameters the same. . Results . Property Details . An overview of the property details data is shown in Table 3 and for the selected locations. For each parameter there is a wide range in values, many variables vary by a factor of more than 2 between maximum and minimum values. The deprivation index, a measure of how deprived an area is using multiple sources such as employment, social class, and availability of cars [19], is also included here but not used in analysis (the higher the value the less deprived). The selected locations also show a range of values for the parameters. The properties in Mumbles have an average price over five times that in Penderry, and two times higher than the Uplands and Maritime regions. There is also a mix in property types, Maritime is mainly flats, whereas Penderry and Morriston are mainly semi-detached and Mumbles mainly detached. Variations in the density of people in the areas also changes significantly, being lowest in the Maritime district which is around seven times denser than Penderry and Mumbles districts. . The 12 parameters on the left are used as input for the model with a target of the average price. . Table . Information of the details of properties in the different Census regions within Swansea using the ‚Äú.describe‚Äù function of pandas. . . Table . Average property details of the selected regions. Note that a higher deprivation index indicates less deprivation of a region. . . The influence that the parameters have on property price can be highlighted by scatter plots and the Pearson correlation coefficient. The Pearson correlation gives the following relationships: 0.5 to 1 strong correlation, 0.3 to 0.49 moderate correlation, 0 to 0.29 low correlation. . For the property data, scatter plots with Pearson correlation displayed in the tile are shown in Figure 9. Most of the property detail variables have a low correlation with price. The exceptions to this are: (1) Number of rooms and bedroom which have a high positive correlation (i.e., more rooms more expensive properties), (2) The percentage of detached properties, positively correlated with price, and (3) the percentage of terraced properties, which is negatively correlated with price and unlike the other two only moderately correlated. . . Figure . Correlation plots of the different details of properties used compared with the averaged property prices. . Using the property details and house prices, predictions on the property prices using different models were made. The results of this analysis are shown in Table 5. . The first model used was a linear regression model, the parameters obtained being shown in Table 6. The results of the model are consistent with the correlation plots, although a little difficult to interpret for the house types. But it is notable that the model estimates a gain of ¬£36,000 per extra room in a property. . With the linear regression model using the property data the mean absolute error (MAE) in property price is ¬£27,000 and the variance score is 0.58. For the KNN regressor model the values are improved for MAE at ¬£24,900 but lower for variance at 0.53. The best model is the Random Forrest Regressor, which has MAE of ¬£24,600 and variance of 0.57. . Based on these results a reasonably accurate predictor of property price can be made from the property details alone. However, there is more to property prices than property details alone. This is illustrated in Figure 10, which shows the difference between property prices and predictions using the Random Forrest model. The figure shows that some areas have a marked difference in property price between model and actual. The regions in the Gower in the west (with negative price difference) suggest the importance of location. . . Figure . Map of Swansea showing the difference in Census regions between predicted property price based on property details. Using the RF model. A negative value indicates the model underestimates the measured property price. . Table . Details of models to predict the property price in Swansea. The mean absolute error (MAE), variance score and details of the parameters used are shown. The models are performed for details about the property alone (Property), details about the location alone (Location), and details about both location and property details (Location + Property). Three different machine learning models are used. . . Table . Results of using a linear regression on the property details to predict property prices. The property price is given as: intercept + parameter multiplied by value. E.g. 7000 + 37000x5 + -2100 + ‚Ä¶. For a 5 room terraced property. . . Location details . To examine the influence of location, several approaches are taken. Given that the Random Forrest Regressor model was more accurate in its prediction than the other two, this model will be used here. . As a first step the longitude and latitude of each region are used as the variables to predict price (recall that 70% of the data is used for training and 30% for testing). As shown in Table 5, with only this position data, this model predicts property prices with a lower mean absolute error than the equivalent RF model using the property details (¬£22,600 versus ¬£24,900 for RF property details). Therefore, this confirms location as the number one classifier on a property and the mantra: ‚ÄúLocation, Location, Location‚Äù. . This description of location (Latitude and Longitude) would be of no use in predicting property prices in different areas outside Swansea, or for understanding the reason for the location effect. Instead, to try to quantify this location effect in Swansea the FourSquare website is used to determine details about the census regions. Two approaches are used: . (1) Finding the distance and density of several selected features from each Census region using the FourSquare ‚Äòsearch‚Äô function [14], . (2) Finding the features nearby a region based on different categories using the FourSquare ‚Äòexplore‚Äô function [15]. . Locations Using Search . From the first of these categories, the correlation of different categories is shown in Figure 11. The Pearson correlation gives the following relationships: 0.5 to 1 strong correlation, 0.3 to 0.49 moderate correlation, 0 to 0.29 low correlation. Most categories have a low correlation. Only supermarket distance has a strong relationship (a positive one, the further away the higher the property prices) and only industry distance has a moderate correlation (a negative one lower house prices closer to industry). Both shopping centre and Schools have a positive correlation at ~0.29, just under the moderate definition. This may be a sign that the categories, or how they were implemented (e.g. distance/frequency), are not the best choice. . The clustering based on these categories is shown in Figure 13. As may be expected due to the nature of the data, the clustering occurs over distinct regions of 1-3 km in size. Although, there are some exceptions to this (e.g. Cluster 29 and cluster 2 where there are 2 separate regions). . . Figure . Correlation plots of the different location details of properties used compared with the averaged property prices. . When the location data is used to predict property prices the predictions (see Table 5) are better than using the property details alone. However, they are also equivalent to those using just the longitude and latitude information. Furthermore, if a 50:50 split between training and test data is used the Longitude-Latitude method outperforms the venue-location based predictions. This would again suggest the venue-location data is not getting to the bottom of the reason of differences in locations. It could either be that the location-venue data is unable to understand the differences and that other data including deprivation and crime statistics may need to be included or that the implementation of the foursquare data needs improvement. The large negative areas in the predicted price difference plots on the West of Swansea found using the property details (Figure 10), are reduced when using the location details (Figure 12) but still exist. . . Figure . Map of Swansea showing the difference in Census regions between predicted property price based on location using the ‚Äòsearch‚Äô function. Using the RF model. A negative value indicates the model underestimates the measured property price. . . Figure . A cluster analysis of locations based on the location details from the ‚Äòsearch‚Äô category. . Locations Using Explore . The location details from explore using the section ‚ÄútopPicks‚Äù, ‚Äútrending‚Äù and ‚Äúshops‚Äù were used for predicting the house price. As is shown in Table 5, the models give the largest mean absolute error of all the models. The data was not meant primarily for this, it was intended for unsupervised Cluster analysis, and so it makes sense that the model is poor at predictions. . The same data was used to predict different Clusters. One of these plots is shown in Figure 14 using the section as ‚Äòtrending‚Äô. The main feature of these plots is the interactivity, which cannot be displayed using an image file. Instead, the interactive plots are found on nbviewer [20]. This gives details about what the clusters represent and to check whether two markers that look the same are the same cluster, when the cluster size is high different clusters can look the same. . . Figure . A cluster analysis of locations based on the location details from the ‚Äòexplore category. . Location and Property details . House price predictions were made using the property details and the location information from the ‚ÄòSearch‚Äô call and are shown in Table 5. As would be expected the mean absolute error is lower than the individual models (¬£17,800 versus ¬£22,800 for location alone), and the variance score is higher (0.78 compared to 0.70 respectively). The error is still relatively high at ¬£17,800, which indicates there is more details to the property price than those given to the model. To illustrate these differences between predictions, the difference between predicted and actual house prices is shown in Figure 15. . . Figure . Map of Swansea showing the difference in Census regions between predicted property price based on property details and location details. Using the RF model. A negative value indicates the model underestimates the measured property price. . Discussion . The Selected Areas . To help understand the data, more detail of the selected regions is shown below. . Table . Details about the selected areas, including model predictions and the most common venues in different categories from clustering. . . . . . Effect of number of rooms . One way to increase a property‚Äôs value that is available to most home-owners is increasing the number of rooms in a property, which can be done by converting an attic or building an extension. Decisions on doing this will be influenced by several factors, and one of the most important is the influence it will have on the property sales price. An indication of the effect of the number of rooms was found by using the Random Forrest (RF) model and shown in Figure 16. . To calculate the value, for each location a range of number of rooms is given to the model whilst maintain all the other features (property details and location), which gives the change in the price of a property with number of rooms (Figure 17). The RF model is based around the use of the sigmoid function, which switches between two values. Due to this the price increase per extra room is not easily found, so instead the difference between the lower and upper house price values is taken and plotted in the figure. . The data should be seen as a guide for homeowners in the regions considering increasing the rooms, or for assessing whether to buy a property. The main trend seen in the figure, which may be expected, is that there is a reasonable correlation between the average property price in a region and this room effect. . . Figure . Map of Swansea showing the difference in Census regions for the effect of the number of rooms in a property. Using the RF model. . . Figure . The effect of number of rooms on property prices in the selected regions. The property effect is the difference between the maximum and minimum prices. . Quantifying the Location Effect . In previous sections it was shown that the location of a property has a greater influence on the price than its property details. To quantify the location effect, a value was obtained of the difference in the average houses price based on its location (Figure 18). This again uses the RF model but this time the property details taken for each region are the average across the districts and using their own location details. The value displayed is the average house price minus the price of the average property in that location. . . Figure . Map of Swansea showing the difference in Census regions for the effect of the number of location of a property. Using the RF model. The value displayed is the average house price minus the price of the average property in the individual locations. Lower values indicate expensive locations. . . Figure . Map of Swansea showing the difference in Census regions between predicted property price based on property details and location details. Using the RF model. A negative value indicates the model underestimates the measured property price. Circles show neighbouring regions with large variations in accuracy of property price predictions. . ## . Is the model and data a good predictor of property prices? . The predictions of the property details and location model, have an error of ¬£18,000. The error can be low in some areas but considerable in others. If the areas with large property prices were excluded, Gower and Marina, the model may be reasonably accurate but with these areas included it is a poor predictor. The level of predictions is of the level of someone with a reasonable understanding of Swansea property prices but would not compete with valuations from estate agents or from those on Zoopla [22]. . The success of the model could be greatly improved by incorporation of crime statistics and deprivation, both of which are readily available. However, refinement is probably required on the property details and perhaps more importantly the location details provided. It is worth recalling that using the latitude and longitude performs slightly better than using the location information from the ‚Äòsearch‚Äô function. So better incorporation of the location details using foursquare or something similar would greatly improve the model. . These modifications could include including the star rating of venues, taking account of the surrounding ruralness, distance to beach and not just one of several recognised beach venues along Swansea Bay. Alternatively, splitting the model into two, one for the Gower and another for the rest could help. In addition, A comparison of the model on a different area, such as Cardiff, would help to understand the underlying trends about what contributes to the location effect. . The data and models are instead best used as a guide for potential buyers/renters or current homeowners. . Getting more out of clustering . The way the clustering and ‚Äòexplore‚Äô location details are used could be refined to get more from this data. In this form they act as a series of maps where differences in the area can be explored from the interactive folium maps. One way to do this would be to convert them into an algorithm that suggests areas to a user based on the wight they put on certain venue types. For example, a user is asked how important different venue types are to them and the model suggests certain areas. . Conclusion . A model to predict property prices in the Swansea region based on details of the property and location relative to nearby venues, was shown to be reasonably accurate. Although, large errors particularly in the West were found in house price predictions. . When broken down to using either the property details or the location, the location was found to be a more accurate indicator of property prices. The characterising of location using the distance and frequency of nearby venues appears to be a significant limitation on the model, as just using longitude and latitude information in the model gave more accurate results. This detail along with not accounting for crime or deprivation would significantly improve predictions. But because of this, and other factors, the reasons why a location is desirable or not has not been established. . Clustering locations based on nearby venues offers a good visual tool for understanding an area. But to make it effective for potential homeowners or others exploring Swansea housing requires more work. . References . [1] https://en.wikipedia.org/wiki/Swansea . [2] https://en.wikipedia.org/wiki/Gower_Peninsula . [3] https://en.wikipedia.org/wiki/Swansea_Marina . [4] https://medium.com/@briskat/england-wales-population-density-heat-map-26a28a2b6091 . [5] https://en.wikipedia.org/wiki/Location,_Location,_Location . [6] https://en.wikipedia.org/wiki/2011_United_Kingdom_census . [7] https://www.nomisweb.co.uk/census/2011 . [8] https://www.nomisweb.co.uk/census/2011/bulk/r2_2 . [9] https://foursquare.com/ . [10] https://borders.ukdataservice.ac.uk/easy_download.html . [11] https://github.com/dMaterialia/SwanseaProperty . [12] https://nbviewer.jupyter.org/ . [13] https://developer.foursquare.com/docs/build-with-foursquare/categories/ . [14] https://developer.foursquare.com/docs/api-reference/venues/search/ . [15] https://developer.foursquare.com/docs/api-reference/venues/explore/ . [16] https://python-visualization.github.io/folium/ . [17] https://docs.microsoft.com/en-us/azure/machine-learning/how-to-select-algorithms . [18] https://scikit-learn.org/stable . [19] https://census.ukdataservice.ac.uk/get-data/related/deprivation.aspx . [20]https://nbviewer.jupyter.org/github/dMaterialia/SwanseaProperty/blob/main/ClusterPlots.ipynb . [21]https://nbviewer.jupyter.org/github/dMaterialia/SwanseaProperty/blob/main/ChoroPloting.ipynb . [22] https://www.zoopla.co.uk/ .",
            "url": "https://thomashsimm.com/2021/08/04/Swansea-House-Price-Report.html",
            "relUrl": "/2021/08/04/Swansea-House-Price-Report.html",
            "date": " ‚Ä¢ Aug 4, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Regular expressions RE",
            "content": ". search for details of text matching a pattern . import re . Find a string within a string . result = re.search(r&quot;aza&quot;,&quot;bazaar&quot;) result . &lt;re.Match object; span=(1, 4), match=&#39;aza&#39;&gt; . result = re.search(r&quot;aza&quot;,&quot;plaza&quot;) result . &lt;re.Match object; span=(2, 5), match=&#39;aza&#39;&gt; . Find a string at the start . re.search(r&quot;^x&quot;,&quot;xenon&quot;) . &lt;re.Match object; span=(0, 1), match=&#39;x&#39;&gt; . The . is a joker button . re.search(r&quot;x.n&quot;,&quot;xenon&quot;) . &lt;re.Match object; span=(0, 3), match=&#39;xen&#39;&gt; . Character classes are inside square brackets . These are OR statements . [a-z] is all lowercase letters [A-Z] is upercase letters [0-9] is numbers . print(re.search(r&quot;[Ppc]ython&quot;,&quot;cython&quot;)) print(re.search(r&quot;[a-z]ython&quot;,&quot;dython&quot;)) print(re.search(r&quot;[a-z]way&quot;,&quot;My way&quot;)) print(re.search(r&quot;[a-z]way&quot;,&quot;Myway&quot;)) #find cloud with letter or number after it print(re.search(r&quot;cloud[a-zA-Z0-9]&quot;,&quot;cloud9&quot;)) print(re.search(r&quot;[a-zA-Z0-9]&quot;,&quot;dy9thon&quot;)) . &lt;re.Match object; span=(0, 6), match=&#39;cython&#39;&gt; &lt;re.Match object; span=(0, 6), match=&#39;dython&#39;&gt; None &lt;re.Match object; span=(1, 5), match=&#39;yway&#39;&gt; &lt;re.Match object; span=(0, 6), match=&#39;cloud9&#39;&gt; &lt;re.Match object; span=(0, 1), match=&#39;d&#39;&gt; . apply a NOT to all in the character class . This uses the pipe class ^ . print(re.search(r&quot;[^a-zA-Z0-9]&quot;,&quot;dy9 thon&quot;)) #this finds an underscore print(re.search(r&quot;[^a-zA-Z0-9]&quot;,&quot;dy9_thon&quot;)) #this includes a not for spaces print(re.search(r&quot;[^a-zA-Z0-9 ]&quot;,&quot;dy9 thon-&quot;)) . &lt;re.Match object; span=(3, 4), match=&#39; &#39;&gt; &lt;re.Match object; span=(3, 4), match=&#39;_&#39;&gt; &lt;re.Match object; span=(8, 9), match=&#39;-&#39;&gt; . find a string OR another one . print(re.search(r&quot;cat|dog&quot;,&quot;I ilke cats&quot;)) . &lt;re.Match object; span=(7, 10), match=&#39;cat&#39;&gt; . Extension of . . .* means can have any length . Greedy . print(re.search(r&quot;p.*n&quot;,&quot;python programming&quot;)) #this finds something starting with py ending n but only a-z chars print(re.search(r&quot;py[a-z]*n&quot;,&quot;python programming&quot;)) . &lt;re.Match object; span=(0, 17), match=&#39;python programmin&#39;&gt; &lt;re.Match object; span=(0, 6), match=&#39;python&#39;&gt; . matches one or more occurences of it | . So o+l looks for ol . print(re.search(r&quot;o+l&quot;,&quot;olly&quot;)) #this fails because there is an i inbetween print(re.search(r&quot;o+l&quot;,&quot;oilly&quot;)) #this finds from 1st o to l print(re.search(r&quot;o+l&quot;,&quot;oolly&quot;)) . &lt;re.Match object; span=(0, 2), match=&#39;ol&#39;&gt; None &lt;re.Match object; span=(0, 3), match=&#39;ool&#39;&gt; . ? zero or one occurence of the character before it . print(re.search(r&quot;p?each&quot;,&quot;To each their own&quot;)) # print(re.search(r&quot;p?each&quot;,&quot;To peach their own&quot;)) # print(re.search(r&quot;p?each&quot;,&quot;Top each their own&quot;)) . &lt;re.Match object; span=(3, 7), match=&#39;each&#39;&gt; &lt;re.Match object; span=(3, 8), match=&#39;peach&#39;&gt; &lt;re.Match object; span=(4, 8), match=&#39;each&#39;&gt; . Check for special characters . Putting before finds the special char . print(re.search(r&quot;.com&quot;,&quot;internet.com&quot;)) # but not here print(re.search(r&quot;.com&quot;,&quot;welcome&quot;)) # Add backslash we get it here print(re.search(r&quot; .com&quot;,&quot;internet.com&quot;)) # and a negative here print(re.search(r&quot; .com&quot;,&quot;welcome&quot;)) . &lt;re.Match object; span=(8, 12), match=&#39;.com&#39;&gt; &lt;re.Match object; span=(2, 6), match=&#39;lcom&#39;&gt; &lt;re.Match object; span=(8, 12), match=&#39;.com&#39;&gt; None . w matches letters number and underscores . d matches digits . s for whitespace characters, space tab or newline . b for word boundaries . print(re.search(r&quot; w*&quot;,&quot;internet.com&quot;)) # and here the whole string print(re.search(r&quot; w*&quot;,&quot;internet99_com&quot;)) . &lt;re.Match object; span=(0, 8), match=&#39;internet&#39;&gt; &lt;re.Match object; span=(0, 14), match=&#39;internet99_com&#39;&gt; . ^ Beginning character . $ End character . # works here print(re.search(r&quot;A.*a&quot;,&quot;Australia&quot;)) # anot here as want to end in a print(re.search(r&quot;A.*a&quot;,&quot;Azerbaijan&quot;)) #add the begin and end chars- works correct for both print(re.search(r&quot;^A.*a$&quot;,&quot;Australia&quot;)) print(re.search(r&quot;^A.*a$&quot;,&quot;Azerbaijan&quot;)) . &lt;re.Match object; span=(0, 9), match=&#39;Australia&#39;&gt; &lt;re.Match object; span=(0, 9), match=&#39;Azerbaija&#39;&gt; &lt;re.Match object; span=(0, 9), match=&#39;Australia&#39;&gt; None . Combine a few . This is for valid variable names . # [a-zA-Z0-9_] then letters, numbers or undercore # *$ end with above pattern=r&quot;^[a-zA-Z_][a-zA-Z0-9_]*$&quot; print(re.search(pattern,&quot;LLnananj_9&quot;)) print(re.search(pattern,&quot;LLnananj_9&quot;)) print(re.search(pattern,&quot;9LLnananj_9&quot;)) . &lt;re.Match object; span=(0, 10), match=&#39;LLnananj_9&#39;&gt; &lt;re.Match object; span=(0, 10), match=&#39;LLnananj_9&#39;&gt; None . capturing groups . # then comma and space # ends with letters number and underscores result = re.search(r&quot;^( w*), ( w*)$&quot;,&quot;Lovelace, Ada&quot;) print(result) print(result.groups()) print(result[0]) print(result[1]) print(result[2]) #print result2 followed by 1 separated by space print( &quot;{} {}&quot;.format(result[2], result[1]) ) . &lt;re.Match object; span=(0, 13), match=&#39;Lovelace, Ada&#39;&gt; (&#39;Lovelace&#39;, &#39;Ada&#39;) Lovelace, Ada Lovelace Ada Ada Lovelace . numeric repetition qualifiers . [a-z]{n} for a repetition of lower case chars n time . print(re.search(r&quot;[a-zA-Z]{5}&quot;,&quot;a ghost&quot;)) # a number but we only get the first print(re.search(r&quot;[a-zA-Z]{5}&quot;,&quot;a scary super ghost&quot;)) . &lt;re.Match object; span=(2, 7), match=&#39;ghost&#39;&gt; &lt;re.Match object; span=(2, 7), match=&#39;scary&#39;&gt; . re.findall will find all instances . and b either side we will get a full word . print(re.findall(r&quot;[a-zA-Z]{5}&quot;,&quot;a scary super ghost&quot;)) # but if we give a longer word? print(re.findall(r&quot;[a-zA-Z]{5}&quot;,&quot;a scary superior ghost&quot;)) # we get part of the superior word # to get just the words we want of 5 long can use b print(re.findall(r&quot; b[a-zA-Z]{5} b&quot;,&quot;a scary superior ghost&quot;)) . [&#39;scary&#39;, &#39;super&#39;, &#39;ghost&#39;] [&#39;scary&#39;, &#39;super&#39;, &#39;ghost&#39;] [&#39;scary&#39;, &#39;ghost&#39;] . [a-z]{n,m} repetition between n and m . [a-z]{n,} repetition of n or more . [a-z]{,n} repetitions of n or less . print(re.findall(r&quot; b[a-zA-Z]{2,4} b&quot;,&quot;a ab abc abcd abcde abcdef&quot;)) #NB b needed otherwise see below print(re.findall(r&quot;[a-zA-Z]{2,4}&quot;,&quot;a ab abc abcd abcde abcdef&quot;)) # 2 and above full word print(re.findall(r&quot; b[a-zA-Z]{2,} b&quot;,&quot;a ab abc abcd abcde abcdef&quot;)) # {,3} up to this many reps print(re.findall(r&quot; b[a-zA-Z]{,3} b&quot;,&quot;a ab abc abcd abcde abcdef&quot;)) . [&#39;ab&#39;, &#39;abc&#39;, &#39;abcd&#39;] [&#39;ab&#39;, &#39;abc&#39;, &#39;abcd&#39;, &#39;abcd&#39;, &#39;abcd&#39;, &#39;ef&#39;] [&#39;ab&#39;, &#39;abc&#39;, &#39;abcd&#39;, &#39;abcde&#39;, &#39;abcdef&#39;] [&#39;a&#39;, &#39;&#39;, &#39;ab&#39;, &#39;&#39;, &#39;abc&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;] . log = &quot;July 31 07:51:48 mycomputer bad_process[12345]: ERROR Performing package upgrade&quot; # has [ followed by digits at least 1 followed by ] regex= r&quot; [( d+) ]&quot; result= re.search(regex,log) print(result[0]) . [12345] . def extract_pid(log_line): regex= r&quot; [( d+) ]&quot; result=re.search(regex,log_line) if result is None: return &quot;&quot; return result[1] print(extract_pid(log)) print(extract_pid(&quot;[cat] sass&quot;)) . 12345 . re.split split based on some characters . re.sub substitute values . With sub . &quot;A&quot;,&quot;B&quot;,&quot;C&quot; A=pattern B=what to put as replacement C=full text . print(re.split(r&quot;[.?!]&quot;,&quot;One sentence. Another one? And the last one!&quot;)) print(re.split(r&quot;([.?!])&quot;,&quot;One sentence. Another one? And the last one!&quot;)) . [&#39;One sentence&#39;, &#39; Another one&#39;, &#39; And the last one&#39;, &#39;&#39;] [&#39;One sentence&#39;, &#39;.&#39;, &#39; Another one&#39;, &#39;?&#39;, &#39; And the last one&#39;, &#39;!&#39;, &#39;&#39;] . print(re.sub(r&quot;[ w.%+-]+@[ w.-]+&quot;,&quot;[REDACTED]&quot;,&quot;Received an email for go_nuts95@my.examle.com&quot;)) . Received an email for [REDACTED] . texta = &quot;Lovelace, Ada&quot; patt = r&quot;^([ w]*), ([ w]*$)&quot; res=re.search(patt, texta) print(res[0],res[1],res[2]) #this says sub with: result2 space result 1 re.sub(patt, r&quot; 2 1&quot;,texta) . Lovelace, Ada Lovelace Ada . &#39;Ada Lovelace&#39; . Some Examples . import re def repeating_letter_a(text): #here we go A or a - followed by a-z or space- followed by a or A result = re.search(r&quot;(a|A)[a-z ]*(a|A)&quot;, text) return result != None print(repeating_letter_a(&quot;banana&quot;)) # True print(repeating_letter_a(&quot;pineapple&quot;)) # False print(repeating_letter_a(&quot;Animal Kingdom&quot;)) # True print(repeating_letter_a(&quot;A is for apple&quot;)) # True . True False True True . # passed has at least 2 groups of alphanumeric characters # (including letters, numbers, and underscores) # separated by one or more whitespace characters. import re def check_character_groups(text): result = re.search(r&quot; w s w&quot;, text) return result != None print(check_character_groups(&quot;One&quot;)) # False print(check_character_groups(&quot;123 Ready Set GO&quot;)) # True print(check_character_groups(&quot;username user_01&quot;)) # True print(check_character_groups(&quot;shopping_list: milk, bread, eggs.&quot;)) # False . False True True False . import re def check_web_address(text): # starts with letters,numbers,underscores # followed by a dot then ends with letters,numbers,underscores pattern = r&quot; w . w*$&quot; result = re.search(pattern, text) return result != None print(check_web_address(&quot;gmail.com&quot;)) # True print(check_web_address(&quot;www@google&quot;)) # False print(check_web_address(&quot;www.Coursera.org&quot;)) # True print(check_web_address(&quot;web-address.com/homepage&quot;)) # False print(check_web_address(&quot;My_Favorite-Blog.US&quot;)) # True . True False True False True . import re def rearrange_name(name): result = re.search(r&quot;^([ w .-]*), ([ w .-]*)$&quot;, name) if result == None: return name return &quot;{} {}&quot;.format(result[2], result[1]) name=rearrange_name(&quot;Kennedy, John F.&quot;) print(name) name=rearrange_name(&quot;Kennedy, John Franklin&quot;) print(name) . John F. Kennedy John Franklin Kennedy . import re def long_words(text): #this says full words ( b) with chars [A-Za-z] repeated 7 times or more {7,} pattern = r&quot; b[A-Za-z]{7,} b&quot; result = re.findall(pattern, text) return result print(long_words(&quot;I like to drink coffee in the morning.&quot;)) # [&#39;morning&#39;] print(long_words(&quot;I also have a taste for hot chocolate in the afternoon.&quot;)) # [&#39;chocolate&#39;, &#39;afternoon&#39;] print(long_words(&quot;I never drink tea late at night.&quot;)) # [] . [&#39;morning&#39;] [&#39;chocolate&#39;, &#39;afternoon&#39;] [] . # to return the uppercase message in parenthesis, after the process id. import re def extract_pid(log_line): regex = r&quot;( [( d+) ]), ( b[A-Z]{2,} b)&quot; # result = re.search(regex, log_line) if result is None: return None print(result) return result[0]#&quot;{} ({})&quot;.format(result[1],result[2]) print(extract_pid(&quot;July 31 07:51:48 mycomputer bad_process[12345]: ERROR Performing package upgrade&quot;)) # 12345 (ERROR) print(extract_pid(&quot;99 elephants in a [cage]&quot;)) # None print(extract_pid(&quot;A string that also has numbers [34567] but no uppercase message&quot;)) # None print(extract_pid(&quot;July 31 08:08:08 mycomputer new_process[67890]: RUNNING Performing backup&quot;)) # 67890 (RUNNING) . None None None None . # as implemented in the following code. # What is the resulting split list? re.split(r&quot;the|a&quot;, &quot;One sentence. Another one? And the last one!&quot;) . [&#39;One sentence. Ano&#39;, &#39;r one? And &#39;, &#39; l&#39;, &#39;st one!&#39;] . import re def transform_record(record): pat = r&quot;( b[A-Za-z ]{2,} b),([0-9-]{2,}),( b[A-Za-z ]{2,} b)&quot; new_record = re.sub(pat,r&quot; 1,+1- 2, 3&quot;,record) return new_record print(transform_record(&quot;Sabrina Green,802-867-5309,System Administrator&quot;)) # Sabrina Green,+1-802-867-5309,System Administrator print(transform_record(&quot;Eli Jones,684-3481127,IT specialist&quot;)) # Eli Jones,+1-684-3481127,IT specialist print(transform_record(&quot;Melody Daniels,846-687-7436,Programmer&quot;)) # Melody Daniels,+1-846-687-7436,Programmer print(transform_record(&quot;Charlie Rivera,698-746-3357,Web Developer&quot;)) # Charlie Rivera,+1-698-746-3357,Web Developer . Sabrina Green,+1-802-867-5309,System Administrator Eli Jones,+1-684-3481127,IT specialist Melody Daniels,+1-846-687-7436,Programmer Charlie Rivera,+1-698-746-3357,Web Developer . import re def multi_vowel_words(text): pattern = r&quot;[A-Za-z]*[aeiou]{3,}[a-z]*&quot; result = re.findall(pattern, text) return result print(multi_vowel_words(&quot;Life is beautiful&quot;)) # [&#39;beautiful&#39;] print(multi_vowel_words(&quot;Obviously, the queen is courageous and gracious.&quot;)) # [&#39;Obviously&#39;, &#39;queen&#39;, &#39;courageous&#39;, &#39;gracious&#39;] print(multi_vowel_words(&quot;The rambunctious children had to sit quietly and await their delicious dinner.&quot;)) # [&#39;rambunctious&#39;, &#39;quietly&#39;, &#39;delicious&#39;] print(multi_vowel_words(&quot;The order of a data queue is First In First Out (FIFO)&quot;)) # [&#39;queue&#39;] print(multi_vowel_words(&quot;Hello world!&quot;)) # [] . [&#39;beautiful&#39;] [&#39;Obviously&#39;, &#39;queen&#39;, &#39;courageous&#39;, &#39;gracious&#39;] [&#39;rambunctious&#39;, &#39;quietly&#39;, &#39;delicious&#39;] [&#39;queue&#39;] [] . import re def transform_comments(line_of_code): patt=r&quot;#{1,}&quot; result = re.sub(patt,&quot;//&quot;,line_of_code) return result print(transform_comments(&quot;### Start of program&quot;)) # Should be &quot;// Start of program&quot; print(transform_comments(&quot; number = 0 ## Initialize the variable&quot;)) # Should be &quot; number = 0 // Initialize the variable&quot; print(transform_comments(&quot; number += 1 # Increment the variable&quot;)) # Should be &quot; number += 1 // Increment the variable&quot; print(transform_comments(&quot; return(number)&quot;)) # Should be &quot; return(number)&quot; . // Start of program number = 0 // Initialize the variable number += 1 // Increment the variable return(number) . import re def convert_phone_number(phone): patt = r&quot;[0-9]{1,}&quot; resa=re.findall(patt,phone) result = re.sub(patt,r&quot;resa[0],resa[1]&quot;,phone) return result print(convert_phone_number(&quot;My number is 212-345-9999.&quot;)) # My number is (212) 345-9999. print(convert_phone_number(&quot;Please call 888-555-1234&quot;)) # Please call (888) 555-1234 print(convert_phone_number(&quot;123-123-12345&quot;)) # 123-123-12345 print(convert_phone_number(&quot;Phone number of Buckingham Palace is +44 303 123 7300&quot;)) # Phone number of Buckingham Palace is +44 303 123 7300 . My number is resa[0],resa[1]-resa[0],resa[1]-resa[0],resa[1]. Please call resa[0],resa[1]-resa[0],resa[1]-resa[0],resa[1] resa[0],resa[1]-resa[0],resa[1]-resa[0],resa[1] Phone number of Buckingham Palace is +resa[0],resa[1] resa[0],resa[1] resa[0],resa[1] resa[0],resa[1] .",
            "url": "https://thomashsimm.com/2020/10/17/Regular-Expressions.html",
            "relUrl": "/2020/10/17/Regular-Expressions.html",
            "date": " ‚Ä¢ Oct 17, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://thomashsimm.com/jupyter/2020/10/10/Template.html",
            "relUrl": "/jupyter/2020/10/10/Template.html",
            "date": " ‚Ä¢ Oct 10, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://thomashsimm.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". . Materials scientist / Data scientist . Data Science Courses Completed . Projects . Swansea house prices- regression model . Building a NN Image Classifier App on the web . Materials science data viz . Some Computer Programming projects . Links . LinkedIn . Google Scholar . Research Gate . Wordpress site . Tweets by ThomasHSimm",
          "url": "https://thomashsimm.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://thomashsimm.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}